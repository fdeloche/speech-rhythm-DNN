{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.470219,
     "end_time": "2021-08-23T21:59:54.063965",
     "exception": false,
     "start_time": "2021-08-23T21:59:51.593746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "from model import *\n",
    "from input import NetworkInput\n",
    "from data import createFeaturesDescription\n",
    "from config import *\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "import itertools\n",
    "\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "from sklearn import cluster\n",
    "\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.024647,
     "end_time": "2021-08-23T21:59:54.114216",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.089569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.037278,
     "end_time": "2021-08-23T21:59:54.178906",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.141628",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "run_with_papermill=False\n",
    "\n",
    "batch_size=16  #16  #512\n",
    "stride=2\n",
    "use_F0=False\n",
    "F0_binary_values=False\n",
    "use_deltas=False\n",
    "\n",
    "hidden_size=128\n",
    "num_layers=2\n",
    "    \n",
    "weights_filename='./logdir/2020-11-20_18-39-24-2_150_multiple_dropout/weights/weights_2020-11-20_18-39-24.h5'\n",
    "weights_name='2_150_multiple_dropout'\n",
    "\n",
    "useExScores=False\n",
    "useRamus=False\n",
    "use1dScores=False\n",
    "useBalancedDataSet=False\n",
    "balanced_dataset_folder='balanced_20_1'\n",
    "\n",
    "evaluate_model=False\n",
    "save_activations=False #save activations to json files  #mode ALL_CELLS or SELECTED_CELLS defined later\n",
    "save_embeddings=True #save embeddings as checkpoint file (for tensorboard)\n",
    "do_similarity_analyses=True\n",
    "compute_confusion_matrix=False\n",
    "\n",
    "embeddings_Hellinger=True  #embeddings based on sqrt of output\n",
    "\n",
    "\n",
    "force_dropout=True #for embeddings, similarity analyses\n",
    "force_dropout_conf_matrix=False #force dropout for confusion matrix \n",
    "#For the examples of outputs, set manually\n",
    "keep_prob=1\n",
    "keep_prob_recurrent=1\n",
    "keep_prob_dense_layer=1\n",
    "\n",
    "#note: others useful parameters in rest of code, e.g. max_examples_language, max_batches for analyses\n",
    "\n",
    "do_pairwise_analysis=False  #pairewise analysis inside every batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.168924,
     "end_time": "2021-08-23T21:59:54.375822",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.206898",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "run_with_papermill = True\n",
    "hidden_size = 150\n",
    "use_deltas = True\n",
    "use_F0 = True\n",
    "F0_binary_values = True\n",
    "useBalancedDataSet = True\n",
    "batch_size = 1\n",
    "evaluate_model = True\n",
    "save_activations = True\n",
    "save_embeddings = False\n",
    "compute_confusion_matrix = False\n",
    "weights_filename = \"./logdir/2021-08-19_14-53-14-2_150_mult_dropout_voiced_unvoiced_bis/weights/weights_2021-08-19_14-53-14.h5\"\n",
    "weights_name = \"weights_2_150_voiced_unvoiced_bis\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.053027,
     "end_time": "2021-08-23T21:59:54.456990",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.403963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not(run_with_papermill):  #manual settings\n",
    "    force_dropout=True\n",
    "    keep_prob=0.7\n",
    "    keep_prob_dense_layer=0.9\n",
    "    \n",
    "    keep_prob_recurrent=0.9\n",
    "    useBalancedDataSet=True\n",
    "    #useRamus=True\n",
    "    \n",
    "        \n",
    "    use_deltas=True\n",
    "    hidden_size=150\n",
    "    \n",
    "    \n",
    "    evaluate_model=False\n",
    "    save_embeddings=False\n",
    "    save_activations=True\n",
    "    do_similarity_analyses=False\n",
    "    \n",
    "    compute_confusion_matrix=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.102754,
     "end_time": "2021-08-23T21:59:54.589989",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.487235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights_config={\n",
    "    'num_steps':32,\n",
    "    'features_description':createFeaturesDescription(F0=use_F0),\n",
    "    'stride':stride\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.131563,
     "end_time": "2021-08-23T21:59:54.752041",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.620478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_weights=True\n",
    "\n",
    "#old models (old inputs)\n",
    "#1D models\n",
    "#weights_filename=\"./models/weights0420/weights20epochs.h5\"\n",
    "#weights_filename, weights_name=\"./models/weights0420b/weights_2020-04-07_17-55-05.h5\", \"weights_2020-04-07\"\n",
    "#weights_config['features_description']=createFeaturesDescription(HRmsValue=False, F0=False)\n",
    "\n",
    "#2D models\n",
    "#weights_filename, weights_name='./models/weights0715-2d-60Hz/weights_2020-07-15_12-15-21.h5', 'weights0715-2d-60Hz'\n",
    "#weights_filename, weights_name='./models/weights0717-2d-60Hz-64/weights_2020-07-17_17-35-40.h5', 'weights0717-2d-60Hz-64'\n",
    "#weights_config['features_description']=createFeaturesDescription(F0=False)\n",
    "#weights_config['stride']=1\n",
    "\n",
    "#weights_filename, weights_name=\"./models/weights0710-3d/weights_2020-07-03_17-40-31.h5\", \"weights0710-3d\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.094304,
     "end_time": "2021-08-23T21:59:54.874907",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.780603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_steps=weights_config[\"num_steps\"]\n",
    "features_description=weights_config['features_description']\n",
    "stride=weights_config['stride']\n",
    "\n",
    "if not(force_dropout or force_dropout_keep_prob_dense_layer):\n",
    "    keep_prob_dense_layer=1\n",
    "    keep_prob=1\n",
    "config=Config(batch_size, num_steps, hidden_size=hidden_size, \n",
    "              num_layers=num_layers,\n",
    "              keep_prob=keep_prob, keep_prob_recurrent=keep_prob_recurrent,\n",
    "              keep_prob_dense_layer=keep_prob_dense_layer)\n",
    "config=completedConfig(config) #take default params for unspecified params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028194,
     "end_time": "2021-08-23T21:59:54.933745",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.905551",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.046059,
     "end_time": "2021-08-23T21:59:55.008002",
     "exception": false,
     "start_time": "2021-08-23T21:59:54.961943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "languages = [\"Danish\", \"Dutch\", \"English\", \"Finnish\",\n",
    "    \"French\", \"German\", \"Hungarian\", \"Italian\",\n",
    "    \"Japanese\", \"Korean\", \"Mandarin\", \"Polish\",\n",
    "    \"Portuguese\", \"Russian\", \"Spanish\",\n",
    "    \"Swedish\", \"Turkish\", \"Estonian\", \"Arabic\", \"Czech\", \"Romanian\",\n",
    "    \"Basque\", \"Catalan\"]  #NB: check that the order of elements is consistent with model\n",
    "\n",
    "#Remove languages with not enough data\n",
    "languages.remove(\"Czech\")\n",
    "languages.remove(\"Romanian\") \n",
    "\n",
    "languages_model=languages\n",
    "#languages=[\"Czech\", \"Romanian\"]\n",
    "languages=[\"Romanian\"]\n",
    "#languages_dataset=languages+['Romanian'] #None -> autodetect,  languages-> same as model (defined above)\n",
    "languages_dataset=None\n",
    "\n",
    "\n",
    "#scores Folder #default: \"./Scores\"\n",
    "assert useExScores^useRamus^use1dScores^useBalancedDataSet , \"choose a unique dataset\"\n",
    "\n",
    "if useExScores:\n",
    "    scores_folder='./ex_Scores'\n",
    "elif useRamus:\n",
    "    scores_folder='./Scores_Ramus'\n",
    "elif use1dScores:\n",
    "    scores_folder='./Scores_1d' \n",
    "else:\n",
    "    scores_folder='./Scores'\n",
    "\n",
    "max_files_evaluation= 2024 #np.inf\n",
    "# FIRST VERSION\n",
    "#languages = ['Danish', 'Russian', 'Mandarin', 'Finnish', 'Dutch', 'English', 'Hungarian', 'Swedish', \n",
    "#             'Italian', 'French', 'Japanese', 'German', 'Portuguese', 'Polish', 'Spanish', 'Korean']\n",
    "\n",
    "\n",
    "sets ={}\n",
    "\n",
    "set_folds=[0]\n",
    "\n",
    "if useExScores or useRamus:\n",
    "    sets_folds={\"test\":[0]}\n",
    "elif useBalancedDataSet:\n",
    "    sets_folds={\"test\":[0]} #subfolder defined later\n",
    "else:\n",
    "    sets_folds = {\"train\" : [0, 1, 2],\n",
    "            \"test\":[3,4],\n",
    "            \"test1\" : [3],\n",
    "            \"test2\" : [4]}\n",
    "\n",
    "initial_sample_length=3*2**14 if useRamus else 10*2**14\n",
    "TFRecords_batch_size=1 if useRamus else 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.166343,
     "end_time": "2021-08-23T21:59:55.202646",
     "exception": false,
     "start_time": "2021-08-23T21:59:55.036303",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_name='train'\n",
    "if set_name in sets_folds:\n",
    "    if useRamus:\n",
    "        subfolders=[\"\"]\n",
    "    elif useBalancedDataSet:\n",
    "        subfolders=[balanced_dataset_folder]\n",
    "    else:\n",
    "        subfolders=[\"fold_{}/\".format(k_fold) for k_fold in sets_folds[set_name]]\n",
    "    sets[set_name] = NetworkInput(config, folder=scores_folder, \n",
    "                         subfolder=subfolders,\n",
    "                stride=stride, verbose=True, for_evaluation=True,                                   \n",
    "                 languages=languages, name=set_name, features_description=features_description,\n",
    "               initial_sample_length=initial_sample_length, TFRecords_batch_size=TFRecords_batch_size, \n",
    "                                  use_deltas=use_deltas, \n",
    "                                 F0_binary_values=F0_binary_values)  #TRAINING SET BUT FOR EVALUATION\n",
    "\n",
    "set_name='test'\n",
    "if useRamus:\n",
    "    subfolders=[\"\"]\n",
    "elif useBalancedDataSet:\n",
    "    subfolders=[balanced_dataset_folder]\n",
    "else:\n",
    "    subfolders=[\"fold_{}/\".format(k_fold) for k_fold in sets_folds[set_name]]\n",
    "sets[set_name] = NetworkInput(config, folder=scores_folder, for_evaluation=True,\n",
    "        subfolder=subfolders,\n",
    "            stride=stride, verbose=True,                                    \n",
    "             languages=languages, languages_model=languages_model, name=set_name, features_description=features_description,\n",
    "               initial_sample_length=initial_sample_length, TFRecords_batch_size=TFRecords_batch_size,\n",
    "                              use_deltas=use_deltas,\n",
    "                             F0_binary_values=F0_binary_values) #autodetect languages\n",
    "\n",
    "\n",
    "'''\n",
    "sets_folds = {\"train\" : [0, 1, 2],\n",
    "        \"test1\" : [3],\n",
    "        \"test2\" : [4]}\n",
    "        \n",
    "        \n",
    "sets ={}\n",
    "\n",
    "sets_folds={\"train\":sets_folds[\"train\"]}\n",
    "\n",
    "for set_name, set_folds in sets_folds.items():\n",
    "    print(\"{} : folds {}\".format(set_name, set_folds))\n",
    "    sets[set_name] = NetworkInput(config, folder='./Scores', \n",
    "                         subfolder=[\"fold_{}/\".format(k_fold) for k_fold in set_folds],\n",
    "                stride=stride, verbose=True,                                    \n",
    "                 languages=languages, name=set_name)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.057316,
     "end_time": "2021-08-23T21:59:55.288762",
     "exception": false,
     "start_time": "2021-08-23T21:59:55.231446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inds_lang_test=list(np.flatnonzero(sets['test'].frequencies))\n",
    "filter_lang_test=np.array(sets['test'].frequencies)>0\n",
    "languages_test= [languages[i] for i in inds_lang_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.031752,
     "end_time": "2021-08-23T21:59:55.352394",
     "exception": false,
     "start_time": "2021-08-23T21:59:55.320642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.538728,
     "end_time": "2021-08-23T21:59:55.922820",
     "exception": false,
     "start_time": "2021-08-23T21:59:55.384092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "networkInput=sets[\"test\"]\n",
    "\n",
    "model=build_model(config, networkInput, return_state=True)  #return_state will be useful to retrieve cell states\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.048117,
     "end_time": "2021-08-23T21:59:56.001288",
     "exception": false,
     "start_time": "2021-08-23T21:59:55.953171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if load_weights:\n",
    "    model.load_weights(weights_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.181724,
     "end_time": "2021-08-23T21:59:56.213594",
     "exception": false,
     "start_time": "2021-08-23T21:59:56.031870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#METRICS\n",
    "#acc_end_seq=AccuracyStateless(networkInput, includeSampleWeights=False)\n",
    "acc_slices=[AccuracyStateless(networkInput, ind_batch_compute=k) for k in range(networkInput.num_slices_by_example)]\n",
    "top3_slices=[TopKAccuracyStateless(networkInput, k=3, ind_batch_compute=j) for j in range(networkInput.num_slices_by_example)]\n",
    "\n",
    "metricsList=[#accuracy_on_last_step, top_k_accuracy_on_last_step_partial(k=3)\n",
    "                    KL_div_on_last_step, cross_entropy_on_last_step]\n",
    "metricsList+=acc_slices\n",
    "metricsList+=top3_slices\n",
    "\n",
    "KLLoss=tf.keras.losses.KLDivergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.98665,
     "end_time": "2021-08-23T21:59:57.230179",
     "exception": false,
     "start_time": "2021-08-23T21:59:56.243529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss=KLLoss, metrics=metricsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027381,
     "end_time": "2021-08-23T21:59:57.281721",
     "exception": false,
     "start_time": "2021-08-23T21:59:57.254340",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": 0.028082,
     "end_time": "2021-08-23T21:59:57.337742",
     "exception": false,
     "start_time": "2021-08-23T21:59:57.309660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "for i, batch in enumerate(networkInput.sliced_batch.take(2)):\n",
    "        pl.plot(batch[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 310.595661,
     "end_time": "2021-08-23T22:05:07.961992",
     "exception": false,
     "start_time": "2021-08-23T21:59:57.366331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if evaluate_model:#EVALUATION\n",
    "    true_nb_batches=networkInput.nbr_batchs*networkInput.num_slices_by_example\n",
    "    max_nb_batches=max_files_evaluation/config.batch_size*networkInput.num_slices_by_example\n",
    "    nb_steps=np.minimum(max_nb_batches, true_nb_batches)\n",
    "    forgetStates=Forget_states_callback(networkInput, model, verbose=False)\n",
    "    callbacksList=[forgetStates]\n",
    "    metrics_end=model.evaluate(networkInput.sliced_batch, verbose=1, steps=nb_steps,callbacks=callbacksList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.871356,
     "end_time": "2021-08-23T22:05:11.732745",
     "exception": false,
     "start_time": "2021-08-23T22:05:09.861389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Test, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.349058,
     "end_time": "2021-08-23T22:05:14.906918",
     "exception": false,
     "start_time": "2021-08-23T22:05:13.557860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_top5(st, y, y_):\n",
    "    languages=languages_model\n",
    "    ind0=np.argmax(y)\n",
    "    ind=np.argsort(-y_)\n",
    "    print(f\"{st}\\nlanguage: {languages[ind0]}\")\n",
    "    st=\" \"\n",
    "    for k in range(7):\n",
    "        st+=f\"{k+1}: {languages[ind[k]]}, \"\n",
    "    st+='\\n'\n",
    "    #for k, lang in enumerate(languages):\n",
    "    #    print(lang)\n",
    "    #    print(y_[k].numpy())\n",
    "    print(st)\n",
    "def gen_yy_():\n",
    "    model.reset_states()\n",
    "    batch=networkInput.sliced_batch.take(networkInput.num_slices_by_example)\n",
    "    #option 1, use  y_=model(x, training=False)\n",
    "    #option 2 (first axis has size batch_size x steps)\n",
    "    #predictions=model.predict(batch, steps = networkInput.num_slices_by_example)\n",
    "    for trueBatch in batch:\n",
    "        x, y, w= trueBatch\n",
    "        filenames=x[2]\n",
    "        y_=model(trueBatch) #, training=False\n",
    "    y=y.numpy()[:,-1]\n",
    "    y_=y_[:,-1]\n",
    "    res=[]\n",
    "    for k in range(networkInput.config.batch_size):\n",
    "        res.append((filenames[k][0].numpy().decode('utf-8'), \n",
    "                                                   y[k], y_[k]))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.276374,
     "end_time": "2021-08-23T22:05:19.049157",
     "exception": false,
     "start_time": "2021-08-23T22:05:15.772783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(min(5, 10//batch_size*5+1)):\n",
    "    batch_yy_ = gen_yy_()\n",
    "    for k in range( min(5, batch_size)):\n",
    "        print_top5(*batch_yy_[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.894381,
     "end_time": "2021-08-23T22:05:20.914009",
     "exception": false,
     "start_time": "2021-08-23T22:05:20.019628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "test audio"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": 0.872463,
     "end_time": "2021-08-23T22:05:22.745031",
     "exception": false,
     "start_time": "2021-08-23T22:05:21.872568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "filename='./ex_Files/English/TIMIT_SA2.WAV'\n",
    "\n",
    "\n",
    "data, samplerate = sf.read(filename)\n",
    "\n",
    "Audio(data, rate=samplerate)\n",
    "#Audio(filename='./ex_Files/English/TIMIT_SA2.WAV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.919393,
     "end_time": "2021-08-23T22:05:24.526434",
     "exception": false,
     "start_time": "2021-08-23T22:05:23.607041",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Retrieve activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2021-08-23T22:05:25.391771",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def cell_st(cell_type):\n",
    "    if cell_type==LSTM_CELL:\n",
    "        return \"lstm\"\n",
    "    elif cell_type==GRU_CELL:\n",
    "        return \"gru\"\n",
    "\n",
    "nb_batchs_iter=networkInput.nbr_batchs if save_activations else 1 #nbr batchs (before splitted) to iter on\n",
    "gen_batchs=iter(networkInput.sliced_batch.take(networkInput.num_slices_by_example*nb_batchs_iter))\n",
    "dic_list=[{} for i in range(nb_batchs_iter*config.batch_size)]\n",
    "i_batch=0\n",
    "\n",
    "#specific activations\n",
    "selected_cells={'lstm_2': {\n",
    "    'cell_states': [3, 4, 92, 115, 116, 121],\n",
    "    'outputs': [3, 4, 92, 115, 116, 121]\n",
    "    },\n",
    "    'lstm_1':{\n",
    "        'outputs': [],\n",
    "        'cell_states': []\n",
    "    }\n",
    "}\n",
    "\n",
    "#mode, save all cells or selected cells\n",
    "SELECTED_CELLS=1\n",
    "ALL_CELLS=0\n",
    "\n",
    "mode=ALL_CELLS\n",
    "\n",
    "while(i_batch<nb_batchs_iter):\n",
    "    #batch=networkInput.sliced_batch.take(networkInput.num_slices_by_example) #defined with generator instead\n",
    "    batch=[next(gen_batchs) for slice_ in range(networkInput.num_slices_by_example)]\n",
    "    for l in range(config.num_layers):  #NB: very inefficient because the network activations are computed several times\n",
    "        layerName=f'{cell_st(config.cell_type)}_{l+1}'\n",
    "        \n",
    "        if l==0: #also add output scores\n",
    "            model.reset_states()\n",
    "\n",
    "            for trueBatch in batch:\n",
    "                x, y, w= trueBatch\n",
    "                filenames=x[2]\n",
    "                y_=model(trueBatch) # training=False\n",
    "            y=y.numpy()[:,-1]\n",
    "            y_=y_.numpy()[:,-1]\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                ind_batch=i_batch*batch_size+i\n",
    "                filename=filenames[i][0].numpy().decode('utf-8')\n",
    "                if useRamus: #HACK\n",
    "                    filename=\"_\".join(filename.split(\"_\")[2:])\n",
    "                dic_list[ind_batch]['filename']=filename\n",
    "                ind0=np.argmax(y[i])\n",
    "                ind=np.argmax(y_[i])\n",
    "                #dic_list[ind_batch]['label']=languages[ind0]\n",
    "                #HACK!!!\n",
    "                dic_list[ind_batch]['label']='Romanian'\n",
    "                \n",
    "                dic_list[ind_batch]['predicted']=languages_model[ind]\n",
    "\n",
    "                dic_list[ind_batch]['activations']={}\n",
    "                if mode==ALL_CELLS:\n",
    "                    dic_list[ind_batch]['scores']={}\n",
    "                    for j, lang in enumerate(languages):\n",
    "                        dic_list[ind_batch]['scores'][lang]=str(y_[i][j])\n",
    "\n",
    "        modelBis = Model(inputs=model.input, outputs=model.get_layer(layerName).output)\n",
    "\n",
    "        modelBis.reset_states()\n",
    "        for trueBatch in batch:\n",
    "            x, y, w= trueBatch\n",
    "            filenames=x[2]\n",
    "            act_seq, act_h, act_c=modelBis(trueBatch)  #training=False\n",
    "        for i in range(batch_size):\n",
    "            ind_batch=i_batch*batch_size+i\n",
    "            dic_list[ind_batch]['activations'][layerName]={}\n",
    "            \n",
    "            if mode==SELECTED_CELLS:\n",
    "                dic_list[ind_batch]['activations'][layerName]['outputs']={}\n",
    "                for j in selected_cells[layerName]['outputs']:\n",
    "                    dic_list[ind_batch]['activations'][layerName]['outputs'][str(j)]=str(act_h.numpy()[i][j])\n",
    "                dic_list[ind_batch]['activations'][layerName]['cell_states']={}\n",
    "                for j in selected_cells[layerName]['cell_states']:\n",
    "                    dic_list[ind_batch]['activations'][layerName]['cell_states'][str(j)]=str(act_c.numpy()[i][j])\n",
    "            elif mode==ALL_CELLS:\n",
    "                dic_list[ind_batch]['activations'][layerName]['outputs']=[str(x) for x in act_h.numpy()[i]]\n",
    "                dic_list[ind_batch]['activations'][layerName]['cell_states']=[str(x) for x in act_c.numpy()[i]]\n",
    "    i_batch+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#save activations to files\n",
    "if save_activations:\n",
    "    for i in range(nb_batchs_iter*batch_size):\n",
    "        dic=dic_list[i]\n",
    "        filename=dic['filename']\n",
    "        mode_text = 'ALL' if mode==ALL_CELLS else 'SELECTED'\n",
    "        jsonFolder=f'./activations/{scores_folder}/{weights_name}_{mode_text}_Romanian/'\n",
    "        os.makedirs(jsonFolder, exist_ok=True)\n",
    "        jsonFilename=f'{jsonFolder}{filename}.json'\n",
    "        with open(jsonFilename, 'w') as f:\n",
    "            json.dump(dic, f, indent=4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#test reading\n",
    "with open('activations/ex_Scores/fold_2_CommonVoice_6a9137ba750e90e_6a9137ba750e90e_slice17.json') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cell_st(cell_type):\n",
    "    if cell_type==LSTM_CELL:\n",
    "        return \"lstm\"\n",
    "    elif cell_type==GRU_CELL:\n",
    "        return \"gru\"\n",
    "\n",
    "nb_batchs_iter=networkInput.nbr_batchs if save_activations else 1 #nbr batchs (before splitted) to iter on\n",
    "gen_batchs=iter(networkInput.sliced_batch.take(networkInput.num_slices_by_example*nb_batchs_iter))\n",
    "#dic_list=[{} for i in range(nb_batchs_iter*config.batch_size)]\n",
    "i_batch=0\n",
    "\n",
    "#specific activations\n",
    "selected_cells={'lstm_2': {\n",
    "    'cell_states': [3, 4, 92, 115, 116, 121],\n",
    "    'outputs': [3, 4, 92, 115, 116, 121]\n",
    "    },\n",
    "    'lstm_1':{\n",
    "        'outputs': [],\n",
    "        'cell_states': []\n",
    "    }\n",
    "}\n",
    "\n",
    "#mode, save all cells or selected cells\n",
    "SELECTED_CELLS=1\n",
    "ALL_CELLS=0\n",
    "\n",
    "mode=ALL_CELLS\n",
    "\n",
    "\n",
    "count_pred=[0 for l in languages_model]\n",
    "\n",
    "while(i_batch<nb_batchs_iter):\n",
    "    #batch=networkInput.sliced_batch.take(networkInput.num_slices_by_example) #defined with generator instead\n",
    "    batch=[next(gen_batchs) for slice_ in range(networkInput.num_slices_by_example)]\n",
    "    model.reset_states()\n",
    "\n",
    "    for trueBatch in batch:\n",
    "        x, y, w= trueBatch\n",
    "        filenames=x[2]\n",
    "        y_=model(trueBatch) # training=False\n",
    "    y=y.numpy()[:,-1]\n",
    "    y_=y_.numpy()[:,-1]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        ind_batch=i_batch*batch_size+i\n",
    "        filename=filenames[i][0].numpy().decode('utf-8')\n",
    "        if useRamus: #HACK\n",
    "            filename=\"_\".join(filename.split(\"_\")[2:])\n",
    "        dic_list[ind_batch]['filename']=filename\n",
    "        ind0=np.argmax(y[i])\n",
    "        ind=np.argmax(y_[i])\n",
    "        #dic_list[ind_batch]['label']=languages[ind0]\n",
    "        #HACK!!!\n",
    "        #dic_list[ind_batch]['label']='Romanian'\n",
    "\n",
    "        #dic_list[ind_batch]['predicted']=languages_model[ind]\n",
    "\n",
    "        count_pred[ind]+=1        \n",
    "        #dic_list[ind_batch]['activations']={}\n",
    "\n",
    "\n",
    "    i_batch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_sort=np.argsort(-np.array(count_pred))\n",
    "for i, ind in enumerate(ind_sort):\n",
    "    print(f'{i+1}: {languages_model[ind]}, predicted {count_pred[ind]} times ({count_pred[ind]/3.2:.2f} %)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if force_dropout:\n",
    "    dropout_flag=f'dropout_{int(round(100*(1-keep_prob_dense_layer)))}'\n",
    "else:\n",
    "    dropout_flag='no_dropout'\n",
    "if embeddings_Hellinger:\n",
    "    Hell_flag='_Hellinger'\n",
    "else:\n",
    "    Hell_flag=''\n",
    "embFolder=f'./embeddings/{scores_folder}/{weights_name}/{dropout_flag}{Hell_flag}/'\n",
    "\n",
    "TRUE_LANGUAGE=0\n",
    "PREDICTED_LANGUAGE=1\n",
    "\n",
    "mode_label = 1    #write both labels either ways, but differs for max nb examples by language strategy\n",
    "\n",
    "max_examples_language=120\n",
    "max_nb_batchs_iter=np.inf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if save_embeddings:\n",
    "\n",
    "    if not os.path.exists(embFolder):\n",
    "        os.makedirs(embFolder)\n",
    "\n",
    "    proj_config = projector.ProjectorConfig()\n",
    "\n",
    "    #proj_config.model_checkpoint_path = embeddings_ckpt_name\n",
    "\n",
    "    #TODO diff tensors with diff. datasets? \n",
    "    embeddings = proj_config.embeddings.add()\n",
    "    # The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
    "    embeddings.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
    "    if mode_label == TRUE_LANGUAGE:\n",
    "        embeddings.metadata_path = 'labels_true.tsv'\n",
    "    elif mode_label == PREDICTED_LANGUAGE:\n",
    "        embeddings.metadata_path = 'labels_predicted.tsv'\n",
    "    projector.visualize_embeddings(embFolder, proj_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# save checkpoint/metadata\n",
    "\n",
    "if save_embeddings:\n",
    "    st_info=''\n",
    "    nb_batchs_iter=min(networkInput.nbr_batchs, max_nb_batchs_iter)\n",
    "    gen_batchs=iter(networkInput.sliced_batch.take(networkInput.num_slices_by_example*nb_batchs_iter))\n",
    "    languages_true_list=[]\n",
    "    languages_predicted_list=[]    \n",
    "    \n",
    "    count_lang=dict([(lang, 0) for lang in languages])\n",
    "    scores=[]\n",
    "    \n",
    "    mean_act=np.zeros(len(languages))  #for normalization purposes if needed\n",
    "    \n",
    "    i_batch=0\n",
    "    while(i_batch<nb_batchs_iter):\n",
    "        above_thr=[count_lang[lang]>=max_examples_language for lang in languages]\n",
    "        if all(above_thr):\n",
    "            break\n",
    "        \n",
    "        #batch=networkInput.sliced_batch.take(networkInput.num_slices_by_example) #defined with generator instead\n",
    "        batch=[next(gen_batchs) for slice_ in range(networkInput.num_slices_by_example)]\n",
    "        model.reset_states()\n",
    "\n",
    "        for trueBatch in batch:\n",
    "            x, y, w= trueBatch\n",
    "            filenames=x[2]\n",
    "            y_=model(trueBatch, training=force_dropout) #training=False\n",
    "        y=y.numpy()[:,-1]\n",
    "        y_=y_.numpy()[:,-1]\n",
    "        \n",
    "        \n",
    "        #mean_act=np.sum(y_, axis=0)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            ind_batch=i_batch*batch_size+i\n",
    "            ind0=np.argmax(y[i])\n",
    "            ind=np.argmax(y_[i])\n",
    "            \n",
    "            lang0=languages[ind0]\n",
    "            lang=languages[ind]\n",
    "            \n",
    "            if mode_label == TRUE_LANGUAGE:\n",
    "                if count_lang[lang0]>=max_examples_language:\n",
    "                    continue\n",
    "                count_lang[lang0] += 1\n",
    "                if count_lang[lang0] == max_examples_language:\n",
    "                    st_info+=f'{lang0}: max examples reached at seq {ind_batch}\\n'\n",
    "                    print(f'{lang0}: max examples reached at seq {ind_batch}')\n",
    "            elif mode_label == PREDICTED_LANGUAGE:\n",
    "                if count_lang[lang]>=max_examples_language:\n",
    "                    continue\n",
    "                count_lang[lang] += 1\n",
    "                if count_lang[lang] == max_examples_language:\n",
    "                    st_info+=f'{lang}: max examples reached at seq {ind_batch}\\n'\n",
    "                    print(f'{lang}: max examples reached at seq {ind_batch}')\n",
    "            languages_true_list.append(lang0)\n",
    "            languages_predicted_list.append(lang)\n",
    "            \n",
    "            if embeddings_Hellinger:\n",
    "                scores.append(np.sqrt(y_[i]))\n",
    "            else:\n",
    "                scores.append(y_[i])\n",
    "                \n",
    "        i_batch+=1\n",
    "    #mean_act/=i_batch\n",
    "    #mean_act_copy=np.copy(mean_act)\n",
    "    \n",
    "    scores_arr=np.stack(scores)\n",
    "    #checkpoint\n",
    "    checkpoint = tf.train.Checkpoint(embedding=tf.Variable(scores_arr))\n",
    "    checkpoint.save(os.path.join(embFolder, \"embeddings.ckpt\"))\n",
    "\n",
    "    #metadata\n",
    "    with open(os.path.join(embFolder, 'labels_true.tsv'), \"w\") as f:\n",
    "        for lang in languages_true_list:\n",
    "            f.write(\"{}\\n\".format(lang))\n",
    "            \n",
    "    with open(os.path.join(embFolder, 'labels_predicted.tsv'), \"w\") as f:\n",
    "        for lang in languages_predicted_list:\n",
    "            f.write(\"{}\\n\".format(lang))\n",
    "            \n",
    "    #save all data to csv\n",
    "    \n",
    "    with open(os.path.join(embFolder, 'data.csv'), \"w\") as f:\n",
    "        csvWriter=csv.writer(f, delimiter='\\t')\n",
    "        csvWriter.writerow(['label_true', 'label_predicted']+languages)\n",
    "        for i in range(len(languages_true_list)):\n",
    "            row=[languages_true_list[i], languages_predicted_list[i]]\n",
    "            row+=list(scores[i])\n",
    "            csvWriter.writerow(row)\n",
    "    with open(f'{embFolder}/info.txt', 'w') as f:\n",
    "        f.write(st_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_batchs=10000//batch_size  #limit analysis to a certain number of batches  #np.inf if no limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Correlation matrix / histogram of activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_similarity_analyses:\n",
    "    nb_batchs_iter=min(networkInput.nbr_batchs, max_batchs)\n",
    "    gen_batchs=iter(networkInput.sliced_batch.take(networkInput.num_slices_by_example*nb_batchs_iter))\n",
    "    \n",
    "    corr_matrix=np.zeros((len(languages), len(languages))) #r coefficient\n",
    "    hist_act=np.zeros((nb_batchs_iter*batch_size, len(languages)))  #non normalized activation scores\n",
    "    hist_act_true_lang=[]\n",
    "    hist_act_predicted_lang=[]\n",
    "    \n",
    "    if do_pairwise_analysis:\n",
    "        pairwise_corr=np.zeros((len(languages), len(languages)))\n",
    "        pairwise_count=np.zeros((len(languages), len(languages)))\n",
    "\n",
    "    i_batch=0\n",
    "    while(i_batch<nb_batchs_iter):\n",
    "        #batch=networkInput.sliced_batch.take(networkInput.num_slices_by_example) #defined with generator instead\n",
    "        batch=[next(gen_batchs) for slice_ in range(networkInput.num_slices_by_example)]\n",
    "        model.reset_states()\n",
    "\n",
    "        for trueBatch in batch:\n",
    "            x, y, w= trueBatch\n",
    "            filenames=x[2]\n",
    "            y_=model(trueBatch, training=force_dropout) # training=False\n",
    "        \n",
    "        y=y.numpy()[:,-1]\n",
    "        y_=y_.numpy()[:,-1]\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            ind0=np.argmax(y[i])\n",
    "            ind=np.argmax(y_[i])\n",
    "\n",
    "            lang0=languages[ind0]\n",
    "            lang=languages[ind]\n",
    "\n",
    "            hist_act_true_lang.append(lang0)\n",
    "            hist_act_predicted_lang.append(lang)\n",
    "\n",
    "        \n",
    "\n",
    "        corr_matrix+= np.sum(np.expand_dims(y_, 1)*np.expand_dims(y_, 2), axis=0)\n",
    "    \n",
    "        #for i in range(batch_size):\n",
    "        #    corr_matrix+=np.outer(y_[i], y_[i])\n",
    "\n",
    "        hist_act[i_batch*batch_size:(i_batch+1)*batch_size]=y_\n",
    "        \n",
    "        \n",
    "        if do_pairwise_analysis:\n",
    "            modelBis = Model(inputs=model.input, outputs=model.get_layer('lstm_2').output)\n",
    "\n",
    "            modelBis.reset_states()\n",
    "            for trueBatch in batch:\n",
    "                x, y, w= trueBatch\n",
    "                filenames=x[2]\n",
    "                act_seq, act_h, act_c=modelBis(trueBatch)  #training=False\n",
    "\n",
    "\n",
    "            for i in range(batch_size):\n",
    "\n",
    "                ind0=np.argmax(y[i])\n",
    "                for j in range(i+1, batch_size):\n",
    "                    ind0bis=np.argmax(y[j])\n",
    "                    ii=ind0\n",
    "                    jj=ind0bis\n",
    "                    corr=np.sum(act_h[i]*act_h[j])\n",
    "                    pairwise_corr[ii][jj]+=corr\n",
    "                    pairwise_corr[jj][ii]+=corr\n",
    "                    pairwise_count[ii][jj]+=1\n",
    "                    pairwise_count[jj][ii]+=1\n",
    "                \n",
    "        \n",
    "                \n",
    "                \n",
    "        i_batch+=1\n",
    "    corr_matrix/=i_batch\n",
    "    dev=np.sqrt(corr_matrix.diagonal())\n",
    "    corr_matrix/=np.outer(dev, dev) #normalization by deviations\n",
    "    hist_act/=np.sum(hist_act, axis=0)\n",
    "    if do_pairwise_analysis:\n",
    "        pairwise_corr/=(pairwise_count+1e-4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Plot histograms\n",
    "\n",
    "sel_lang_labels=('Dutch',\n",
    " 'English',\n",
    " 'German',\n",
    " 'Italian',\n",
    " 'Spanish',\n",
    " 'Russian',\n",
    " 'Portuguese',\n",
    " 'Catalan')\n",
    "sel_lang_inds=[14, 20, 12, 13,5, 1 ] #7 Italian #2 English #20 Catalan #19 Basque\n",
    "#sel_lang_inds=[1,2,5, 13,12,4,16,18, 15,11, 14,19,20,7, 17, 0, 3, 6, 8, 9, 10]\n",
    "sel_lang_labels=[languages[ind] for ind in sel_lang_inds]\n",
    "sel_lang=dict(zip(sel_lang_labels, sel_lang_inds))\n",
    "\n",
    "m=40\n",
    "\n",
    "permut=np.arange(m)\n",
    "#permut=[6,7,4,8,1,2,6,0,5, 3]\n",
    "\n",
    "for label, ind in sel_lang.items():\n",
    "    pl.figure(figsize=(15, 1))\n",
    "    hist_lang= np.array([hist_act[ind2][ind] for ind2 in permut])\n",
    "    #NORMALIZE?\n",
    "    #hist_lang/=np.sum(hist_lang)\n",
    "    pl.bar(np.arange(m), hist_lang) \n",
    "\n",
    "    \n",
    "    #HACK rounded boxes\n",
    "    \n",
    "    from matplotlib.patches import FancyBboxPatch\n",
    "    \n",
    "    new_patches = []\n",
    "    ax=pl.gca()\n",
    "    for patch in reversed(ax.patches):\n",
    "        bb = patch.get_bbox()\n",
    "        color=patch.get_facecolor()\n",
    "        p_bbox = FancyBboxPatch((bb.xmin, bb.ymin),\n",
    "                            abs(bb.width), abs(bb.height),\n",
    "                            boxstyle=f\"round, pad=0, rounding_size={min(0.1, bb.height*15)}\",\n",
    "                            ec=\"none\", fc=color,\n",
    "                            mutation_aspect=0.04)\n",
    "        p_bbox2 = FancyBboxPatch((bb.xmin, bb.ymin),\n",
    "                            abs(bb.width), abs(0.5*bb.height),\n",
    "                            boxstyle=f\"round, pad=0, rounding_size=0.001\",\n",
    "                            ec=\"none\", fc=color,\n",
    "                            mutation_aspect=0.04)      \n",
    "        patch.remove()\n",
    "        new_patches.append(p_bbox)\n",
    "                                 \n",
    "        new_patches.append(p_bbox2)\n",
    "    for patch in new_patches:\n",
    "        ax.add_patch(patch)\n",
    "    pl.ylim([0, 0.02])\n",
    "    pl.ylabel(label)\n",
    "    \n",
    "    ax.set_xticks(np.arange(m)-0.5)\n",
    "    ax.set_xticklabels( [hist_act_true_lang[ind2] for ind2 in permut], rotation=45 )\n",
    "\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#proximity measures based on activation histograms\n",
    "if do_similarity_analyses:\n",
    "    prox_d_kl=np.zeros((len(languages), len(languages)))\n",
    "    prox_d_hell=np.zeros((len(languages), len(languages)))\n",
    "    prox_d_bhat=np.zeros((len(languages), len(languages)))\n",
    "    for i in range(len(languages)):\n",
    "        for j in range(len(languages)):\n",
    "            p_i=hist_act[:, i]+1e-8\n",
    "            p_j=hist_act[:, j]+1e-8\n",
    "            prox_d_kl[i][j]=np.sum(p_i*np.log2(p_i/p_j))\n",
    "            prox_d_kl[j][i]=np.sum(p_j*np.log2(p_j/p_i))\n",
    "            prox_d_hell[i][j]=np.sqrt(np.sum((np.sqrt(p_i)-np.sqrt(p_j))**2))\n",
    "            prox_d_hell[j][i]=prox_d_hell[i][j]\n",
    "                        \n",
    "            prox_d_bhat[i][j]=-2*np.log2(np.sum(np.sqrt(p_i*p_j)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_distance_matrix(dm, classes, normalize=False, title='Distance matrix', cmap=pl.cm.Blues, vmin=0, vmax=4, invert_colors=False):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "\n",
    "    if normalize:\n",
    "        dm = dm.astype('float') / (0.00001+dm.sum(axis=1)[:, np.newaxis])\n",
    "        dm = np.round(dm*100, decimals=2)\n",
    "\n",
    "    #print(cm)\n",
    "    \n",
    "    #pl.imshow(-np.log2(dm), interpolation='nearest', cmap=cmap, vmax=-3,vmin=-5)\n",
    "    #pl.imshow(-dm, interpolation='nearest', cmap=cmap, vmax=-8,vmin=-17)\n",
    "    if not(invert_colors):\n",
    "        pl.imshow(-dm, interpolation='nearest', cmap=cmap, vmin=-vmax, vmax=-vmin)\n",
    "    else:\n",
    "        pl.imshow(dm, interpolation='nearest', cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    \n",
    "    pl.title(title)\n",
    "    #pl.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    pl.xticks(tick_marks, classes, rotation=45)\n",
    "    pl.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = dm.max()*(1-2*invert_colors)  / 2.\n",
    "    for i, j in itertools.product(range(dm.shape[0]), range(dm.shape[1])):\n",
    "        pl.text(j, i, int(dm[i,j]*100)*1./100,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if dm[i, j]*(1-2*invert_colors) < thresh else \"black\")\n",
    "\n",
    "    #pl.tight_layout()\n",
    "    pl.ylabel('Label1 (reference)')\n",
    "    pl.xlabel('Label2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def permut_mat(mat, permut=[1,2,5, 13,12,4,16,18, 15,11, 14,19,20,7, 17, 0, 3, 6, 8, 9, 10]):\n",
    "    conf_matrix=mat\n",
    "    conf_matrix_permut=np.zeros_like(conf_matrix)\n",
    "    for i, ind in enumerate(permut):\n",
    "        conf_matrix_permut[i]=conf_matrix[ind][permut]\n",
    "    languages_permut=[languages[ind] for ind in permut]\n",
    "    return languages_permut, conf_matrix_permut\n",
    "\n",
    "\n",
    "if do_similarity_analyses:\n",
    "    #pl.figure(figsize=(10, 10))\n",
    "    #plot_distance_matrix(corr_matrix, languages, title='Correlation matrix', vmin=0.05, vmax=0.5)\n",
    "    \n",
    "    pl.figure(figsize=(10, 10))\n",
    "    languages_permut, prox_d_hell_permut=permut_mat(prox_d_hell)\n",
    "    plot_distance_matrix(prox_d_hell_permut, languages_permut, title='Dissimilarity matrix (Hellinger distance)', vmin=0.8, vmax=1.3)\n",
    "        \n",
    "    pl.figure(figsize=(10, 10))\n",
    "    \n",
    "    languages_permut, prox_d_kl_permut=permut_mat(prox_d_kl)\n",
    "    plot_distance_matrix(prox_d_kl_permut, languages_permut, title='Dissimilarity matrix (KL div)', vmin=3, vmax=15)\n",
    "    \n",
    "\n",
    "    pl.figure(figsize=(10, 10))\n",
    "    plot_distance_matrix( (prox_d_kl_permut-prox_d_kl_permut.T)/(prox_d_kl_permut+1e-3)*100, languages_permut, title='KL div, diff transpose', vmin=-20, vmax=20, invert_colors=True)\n",
    "    \n",
    "\n",
    "    pl.figure(figsize=(10, 10))\n",
    "    languages_permut, prox_d_bhat_permut=permut_mat(prox_d_bhat)\n",
    "    plot_distance_matrix(prox_d_bhat_permut, languages_permut, title='Dissimilarity matrix (Bhattacharyya distance)', vmin=0, vmax=8)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "*Multidimensional scaling (nonmetric scaling)*\n",
    "\n",
    "See http://scikit-learn.org/stable/modules/manifold.html#multidimensional-scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn import cluster\n",
    "from sklearn import manifold\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "mds = manifold.MDS(n_components=2, metric=True, \n",
    "                   verbose=0, dissimilarity='precomputed', n_init=10) \n",
    "\n",
    "#NON METRIC\n",
    "#mds = manifold.MDS(n_components=2, metric=False, \n",
    "#                   n_init=30, max_iter=300, \n",
    "#                   verbose=0, eps=0.001, dissimilarity='precomputed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dist_measure_str = \"Hellinger distance\"\n",
    "dm=prox_d_hell\n",
    "\n",
    "dist_measure_str = \"(symm.) KL divergence\"\n",
    "dm=prox_d_kl\n",
    "\n",
    "\n",
    "languages2=languages\n",
    "'''\n",
    "print('not shown: Hungarian, Finnish, (and Polish?)')\n",
    "#HACK delete hungarian\n",
    "dm=np.delete(dm, 6, axis=0)\n",
    "dm=np.delete(dm, 6, axis=1)\n",
    "languages2=languages[0:6]+languages[7::]\n",
    "\n",
    "#HACK delete Finnish\n",
    "dm=np.delete(dm, 3, axis=0)\n",
    "dm=np.delete(dm, 3, axis=1)\n",
    "languages2=languages2[0:3]+languages2[4::]\n",
    "\n",
    "#HACK delete Polish\n",
    "dm=np.delete(dm, 9, axis=0)\n",
    "dm=np.delete(dm, 9, axis=1)\n",
    "languages2=languages2[0:9]+languages2[10::]\n",
    "'''\n",
    "\n",
    "coord_pts = mds.fit_transform((dm.T+dm)/2) #symmetrize if necessary\n",
    "delta = 0.01\n",
    "\n",
    "fig = pl.figure(figsize=(10,10))\n",
    "ax = pl.gca()\n",
    "ax.scatter(coord_pts[:,0], coord_pts[:,1])\n",
    "\n",
    "for i, txt in enumerate(languages2):\n",
    "    ax.annotate(txt, coord_pts[i]+(delta, delta))\n",
    "\n",
    "pl.title(\"Metric multidimensional scaling (computed with {})\".format(dist_measure_str))\n",
    "\n",
    "pl.plot()\n",
    "\n",
    "\n",
    "stress=np.sqrt(mds.stress_/(np.sum(dm**2)/2))\n",
    "\n",
    "dm2=pairwise_distances(coord_pts)\n",
    "stress2=np.sqrt(np.sum((dm2-dm)**2)/np.sum(dm**2))\n",
    "\n",
    "print(f'stress : {stress}')\n",
    "\n",
    "print(f'stress : {stress2}')\n",
    "\n",
    "for i in range(len(languages2)):\n",
    "    act_diss=dm[i]\n",
    "    diff=(dm2[i]-dm[i])\n",
    "    lang=languages2[i]\n",
    "    print(f'{lang.rjust(10)}\\t actual dissimilarity: {np.sum(act_diss):.3f} \\t difference: {np.sum(np.abs(diff)):.3f} \\t percent diff.: {np.sum(np.abs(diff))/np.sum(act_diss)*100:.2f} %')\n",
    "\n",
    "'''\n",
    "coord_pts = mds.fit_transform((dm_selected_modified.T+dm_selected_modified)/2)\n",
    "\n",
    "fig = pl.figure(figsize=(10,10))\n",
    "ax = pl.gca()\n",
    "ax.scatter(coord_pts[:,0], coord_pts[:,1])\n",
    "\n",
    "for i, txt in enumerate(selected_languages):\n",
    "    ax.annotate(txt, coord_pts[i]+(delta, delta))\n",
    "\n",
    "dist_measure_str = \"KL divergence\" if dist_measure == D_KL else \"Hellinger distance\"\n",
    "pl.title(\"Metric multidimensional scaling (computed with {})\".format(dist_measure_str))\n",
    "\n",
    "pl.plot()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Note: the 'hole' : Japanese, Mandarin, Korean, Polish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.imshow(-dm2)\n",
    "pl.colorbar()\n",
    "pl.figure()\n",
    "pl.imshow(-dm)\n",
    "pl.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Some clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delta=10.\n",
    "n_clusters = 6\n",
    "spec_clustering = cluster.SpectralClustering(n_clusters=n_clusters,\n",
    "                                          affinity=\"precomputed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if do_similarity_analyses:\n",
    "    dm=(prox_d_kl+prox_d_kl.T)/2\n",
    "\n",
    "    similarity_matrix=np.exp(- dm**2 / (2. * delta ** 2))\n",
    "    clusters = spec_clustering.fit_predict(similarity_matrix)\n",
    "    print(\"All languages : \")\n",
    "    for i in range(n_clusters):\n",
    "        cluster_labels = [languages[j] for j in np.where(clusters == i)[0]]\n",
    "        print(\"> cluster {} : {} \\n\".format(i, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt=pl\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack([model.children_, model.distances_,\n",
    "                                      counts]).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "    \n",
    "if do_similarity_analyses:\n",
    "    # Ward based on hist_act\n",
    "    # setting distance_threshold=0 ensures we compute the full tree.\n",
    "    model_clustering = cluster.AgglomerativeClustering(distance_threshold=0, n_clusters=None) #affinity='precomputed' \n",
    "\n",
    "\n",
    "    #XXX hist_act or sqrt hist_act ??\n",
    "    model_clustering = model_clustering.fit(np.sqrt(hist_act.T))   \n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "\n",
    "\n",
    "    # plot the dendrogram\n",
    "\n",
    "    def labels_dendogram(k):\n",
    "        return languages[k]\n",
    "\n",
    "    plot_dendrogram(model_clustering, leaf_label_func=labels_dendogram, leaf_rotation=70)  #, truncate_mode='level', p=3\n",
    "    plt.xlabel(\"Ward + Hellinger distance\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    #DM  #try different linkage methods\n",
    "\n",
    "    model_clustering = cluster.AgglomerativeClustering(distance_threshold=0, n_clusters=None, affinity='precomputed', linkage='complete' )\n",
    "\n",
    "    dm=(prox_d_kl+prox_d_kl.T)/2\n",
    "\n",
    "    model_clustering = model_clustering.fit(dm)   #if dm, needs to add affinity='precomputed' and change linkage method\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "\n",
    "\n",
    "    # plot the dendrogram\n",
    "\n",
    "    def labels_dendogram(k):\n",
    "        return languages[k]\n",
    "\n",
    "    plot_dendrogram(model_clustering, leaf_label_func=labels_dendogram, leaf_rotation=70)  #, truncate_mode='level', p=3\n",
    "    plt.xlabel(\"Complete linkage based on symmetrized D_KL \")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Bhattacharyya distance\n",
    "        \n",
    "    model_clustering = cluster.AgglomerativeClustering(distance_threshold=0, n_clusters=None, affinity='precomputed', linkage='complete' )\n",
    "    dm=prox_d_bhat\n",
    "    #dm+=16*prox_d_hell\n",
    "\n",
    "    model_clustering = model_clustering.fit(dm)   #if dm, needs to add affinity='precomputed' and change linkage method\n",
    "    plt.title('Hierarchical Clustering Dendrogram')\n",
    "\n",
    "\n",
    "    # plot the dendrogram\n",
    "\n",
    "\n",
    "    plot_dendrogram(model_clustering, leaf_label_func=labels_dendogram, leaf_rotation=70)  #, truncate_mode='level', p=3\n",
    "    plt.xlabel(\"Complete linkage based on Bhattacharyya distance \")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "from scipy.cluster.hierarchy import linkage  #test scipy algorithm instead\n",
    "\n",
    "Z = linkage(np.sqrt(hist_act.T), method='complete')\n",
    "dn = dendrogram(Z, leaf_label_func=labels_dendogram, leaf_rotation=70)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_batchs=np.infty\n",
    "\n",
    "if compute_confusion_matrix:\n",
    "    nb_batchs_iter=min(networkInput.nbr_batchs, max_batchs)\n",
    "    gen_batchs=iter(networkInput.sliced_batch.take(networkInput.num_slices_by_example*nb_batchs_iter))\n",
    "    \n",
    "    #lang_true=[]\n",
    "    #lang_predicted=[]\n",
    "    \n",
    "    conf_matrix=np.zeros((len(languages), len(languages)))\n",
    "    conf_matrix_filtered=np.zeros((len(languages_test), len(languages_test)))\n",
    "    \n",
    "    i_batch=0\n",
    "    while(i_batch<nb_batchs_iter):\n",
    "        #batch=networkInput.sliced_batch.take(networkInput.num_slices_by_example) #defined with generator instead\n",
    "        batch=[next(gen_batchs) for slice_ in range(networkInput.num_slices_by_example)]\n",
    "        model.reset_states()\n",
    "\n",
    "        for trueBatch in batch:\n",
    "            x, y, w= trueBatch\n",
    "            filenames=x[2]\n",
    "            y_=model(trueBatch, training=force_dropout_conf_matrix)  #training=False\n",
    "        \n",
    "        y=y.numpy()[:,-1]\n",
    "        y_=y_.numpy()[:,-1]\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            ind0=np.argmax(y[j])\n",
    "            ind=np.argmax(y_[j])\n",
    "\n",
    "            lang0=languages[ind0]\n",
    "            lang=languages[ind]\n",
    "            \n",
    "            #lang_true.append(lang0)\n",
    "            #lang_predicted.append(lang)\n",
    "            \n",
    "            conf_matrix[ind0][ind]+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            ind0bis=np.argmax(y[j][inds_lang_test])\n",
    "            indbis=np.argmax(y_[j][inds_lang_test])\n",
    "\n",
    "            conf_matrix_filtered[ind0bis][indbis]+=1\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        i_batch+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "if compute_confusion_matrix:\n",
    "    \n",
    "    #plot_distance_matrix(conf_matrix, languages, vmin=0, vmax=20,  normalize=True, invert_colors=True)\n",
    "\n",
    "    permut=[1,2,5, 13,12,4,16,18, 15,11, 14,19,20,7, 17, 0, 3, 6, 8, 9, 10]\n",
    "    \n",
    "    conf_matrix_permut=np.zeros_like(conf_matrix)\n",
    "    for i, ind in enumerate(permut):\n",
    "        conf_matrix_permut[i]=conf_matrix[ind][permut]\n",
    "    languages_permut=[languages[ind] for ind in permut]\n",
    "    \n",
    "    if useRamus:\n",
    "        plot_distance_matrix(conf_matrix_filtered, languages_test, vmin=0, vmax=5, invert_colors=True,title='Confusion matrix')\n",
    "        permut=[0,1,2,5,3,6,7, 4]\n",
    "        conf_matrix_filtered_permut=np.zeros_like(conf_matrix_filtered)\n",
    "        for i, ind in enumerate(permut):\n",
    "            conf_matrix_filtered_permut[i]=conf_matrix_filtered[ind][permut]\n",
    "        languages_test_permut=[languages_test[ind] for ind in permut]\n",
    "        pl.figure(figsize=(6,6))\n",
    "        \n",
    "        pl.ylim([7.5, -0.5])\n",
    "        plot_distance_matrix(conf_matrix_filtered_permut, languages_test_permut , vmin=0, vmax=5, invert_colors=True, title='Confusion matrix')\n",
    "        pl.figure(figsize=(6,6))\n",
    "        plot_distance_matrix(conf_matrix_filtered_permut, languages_test_permut , normalize=True, vmin=0, vmax=50, invert_colors=True, title='Confusion matrix')\n",
    "\n",
    "        pl.ylim([7.5, -0.5])\n",
    "    else:\n",
    "        pl.figure(figsize=(15,15))\n",
    "        plot_distance_matrix(conf_matrix_permut, languages_permut , normalize=True, vmin=0, vmax=10, invert_colors=True, title='Confusion matrix')\n",
    "     \n",
    "        pl.ylim([20.5, -0.5])"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Format de la Cellule Texte Brut",
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "Evaluate model.ipynb",
   "output_path": "evaluate_2_150_voiced_unvoiced_bis_activations.ipynb",
   "parameters": {
    "F0_binary_values": true,
    "batch_size": 1,
    "compute_confusion_matrix": false,
    "evaluate_model": true,
    "hidden_size": 150,
    "run_with_papermill": true,
    "save_activations": true,
    "save_embeddings": false,
    "useBalancedDataSet": true,
    "use_F0": true,
    "use_deltas": true,
    "weights_filename": "./logdir/2021-08-19_14-53-14-2_150_mult_dropout_voiced_unvoiced_bis/weights/weights_2021-08-19_14-53-14.h5",
    "weights_name": "weights_2_150_voiced_unvoiced_bis"
   },
   "start_time": "2021-08-23T21:59:50.690865",
   "version": "2.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
