{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from model import *\n",
    "from input import NetworkInput\n",
    "from config import *\n",
    "\n",
    "\n",
    "from data import createFeaturesDescription\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_txt_files=True\n",
    "\n",
    "from IPython.utils.io import Tee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size=512\n",
    "num_steps=32\n",
    "\n",
    "keep_prob=1\n",
    "keep_prob_recurrent=1\n",
    "keep_prob_dense_layer=1\n",
    "\n",
    "data_augmentation=True\n",
    "name_train='train'\n",
    "\n",
    "hidden_size=128\n",
    "num_layers=2\n",
    "\n",
    "stride = 2\n",
    "\n",
    "run_with_papermill=False #if true, verbose mode will be set to 2 (1 line/epoch)\n",
    "\n",
    "kernel_regularizer_l2=0\n",
    "recurrent_regularizer_l2=0\n",
    "use_deltas=False\n",
    "\n",
    "nb_epochs=30\n",
    "\n",
    "train_model=True\n",
    "save_weights=True\n",
    "\n",
    "\n",
    "load_weights=False\n",
    "weights_filename=\"./logdir/2020-10-29_17-50-58-size_3_128/weights/weights_2020-10-29_17-50-58.h5\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "config=Config(batch_size, num_steps, keep_prob=keep_prob, keep_prob_recurrent=keep_prob_recurrent, \n",
    "              hidden_size=hidden_size, num_layers=num_layers, \n",
    "              kernel_regularizer_l2=kernel_regularizer_l2, recurrent_regularizer_l2=recurrent_regularizer_l2)\n",
    "config=completedConfig(config) #take default params for unspecified params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_description=createFeaturesDescription(F0=False) #Features RMS, RMS HP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = [\"Danish\", \"Dutch\", \"English\", \"Finnish\",\n",
    "    \"French\", \"German\", \"Hungarian\", \"Italian\",\n",
    "    \"Japanese\", \"Korean\", \"Mandarin\", \"Polish\",\n",
    "    \"Portuguese\", \"Russian\", \"Spanish\",\n",
    "    \"Swedish\", \"Turkish\", \"Estonian\", \"Arabic\", \"Czech\", \"Romanian\",\n",
    "    \"Basque\", \"Catalan\"]\n",
    "\n",
    "#Remove languages with not enough data\n",
    "#languages.remove(\"Czech\")\n",
    "#languages.remove(\"Romanian\")\n",
    "\n",
    "# FIRST VERSION\n",
    "#languages = ['Danish', 'Russian', 'Mandarin', 'Finnish', 'Dutch', 'English', 'Hungarian', 'Swedish', \n",
    "#             'Italian', 'French', 'Japanese', 'German', 'Portuguese', 'Polish', 'Spanish', 'Korean']\n",
    "\n",
    "sets ={}\n",
    "\n",
    "set_folds=[0]\n",
    "\n",
    "sets_folds = {\"train\" : [0, 1, 2],\n",
    "        \"test\":[3,4],\n",
    "        \"test1\" : [3],\n",
    "        \"test2\" : [4]}\n",
    "\n",
    "set_name='train'\n",
    "sets[set_name] = NetworkInput(config, folder='./Scores', \n",
    "                     subfolder=[\"fold_{}/\".format(k_fold) for k_fold in sets_folds[set_name]],\n",
    "            stride=stride, verbose=True,                                    \n",
    "             languages=languages, name=set_name, features_description=features_description, \n",
    "                             data_augmentation=data_augmentation, use_deltas=use_deltas)\n",
    "\n",
    "set_name='test'\n",
    "sets[set_name] = NetworkInput(config, folder='./Scores', for_evaluation=True,\n",
    "        subfolder=[\"fold_{}/\".format(k_fold) for k_fold in sets_folds[set_name]],\n",
    "            stride=stride, verbose=True,                                    \n",
    "             languages=None, languages_model=languages, name=set_name,\n",
    "                features_description=features_description, use_deltas=use_deltas) #autodetect languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN\n",
    "networkInput=sets[\"train\"]\n",
    "set_name=\"train\"\n",
    "\n",
    "#TEST\n",
    "#networkInput=sets[\"test\"]\n",
    "#set_name=\"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_info={lang:{} for lang in languages}\n",
    "\n",
    "for batch in networkInput.processed_full_batch:\n",
    "    featuresTensor_strided, language, speaker_joined, database, filename = batch\n",
    "    for lang, spk, database in zip(language, speaker_joined, database):\n",
    "        lang=lang.numpy().decode('utf8')\n",
    "        spk=spk.numpy().decode('utf8')\n",
    "        database=database.numpy().decode('utf8')\n",
    "        if not (database in dic_info[lang]):\n",
    "            dic_info[lang][database]={}\n",
    "        if not (spk in dic_info[lang][database]):\n",
    "            dic_info[lang][database][spk]=1\n",
    "        else:\n",
    "            dic_info[lang][database][spk]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_txt_files:\n",
    "    t=Tee(f'./Files/info_{set_name}_set_speakers.csv')\n",
    "\n",
    "print(' \\t '.join(['Language', 'librivox', 'CommonVoice', 'voxforge', 'WLI', 'tatoeba']))\n",
    "for lang, dic_database in dic_info.items():\n",
    "    st=f'{lang}'\n",
    "    for database in ['librivox', 'CommonVoice', 'voxforge', 'WLI', 'tatoeba']:\n",
    "        if database in dic_database:\n",
    "            st+=f' \\t {len(dic_database[database])}'  \n",
    "        else:\n",
    "            st+=' \\t '\n",
    "    print(st)\n",
    "    \n",
    "if write_txt_files:\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if write_txt_files:\n",
    "    t=Tee(f'./Files/info_{set_name}_set_files.csv')\n",
    "\n",
    "print(' \\t '.join(['Language', 'librivox', 'CommonVoice', 'voxforge', 'WLI', 'tatoeba']))\n",
    "for lang, dic_database in dic_info.items():\n",
    "    st=f'{lang}'\n",
    "    for database in ['librivox', 'CommonVoice', 'voxforge', 'WLI', 'tatoeba']:\n",
    "        if database in dic_database:\n",
    "            st+=f' \\t {sum([nb_files for nb_files in dic_database[database].values()])}'  \n",
    "        else:\n",
    "            st+=' \\t '\n",
    "    print(st)\n",
    "if write_txt_files:\n",
    "    t.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
