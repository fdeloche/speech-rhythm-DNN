{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.387448,
     "end_time": "2021-10-13T17:46:16.798990",
     "exception": false,
     "start_time": "2021-10-13T17:46:16.411542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "\n",
    "#mpl.rcParams['text.usetex'] = True\n",
    "\n",
    "'''mpl.rcParams['axes.titlesize']=24\n",
    "mpl.rcParams['axes.labelsize']= 20\n",
    "mpl.rcParams['lines.linewidth']= 3\n",
    "mpl.rcParams['font.size']= 14\n",
    "mpl.rcParams['lines.markersize']=  10\n",
    "mpl.rcParams['xtick.labelsize']=  16\n",
    "mpl.rcParams['ytick.labelsize']=  16\n",
    "'''\n",
    "\n",
    "\n",
    "mpl.rcParams['axes.titlesize'] = 20  \n",
    "mpl.rcParams['axes.labelsize'] = 16 \n",
    "mpl.rcParams['lines.linewidth'] = 2.5 \n",
    "mpl.rcParams['font.size'] = 12 \n",
    "mpl.rcParams['lines.markersize'] = 8  \n",
    "mpl.rcParams['xtick.labelsize'] = 14 \n",
    "mpl.rcParams['ytick.labelsize'] = 14\n",
    "    \n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "\n",
    "pl.style.use('seaborn-deep')\n",
    "\n",
    "\n",
    "from scipy.stats.stats import pearsonr\n",
    "from create_dictionaries import create_dictionaries\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "\n",
    "ALL_CELLS=0\n",
    "SELECTED_CELLS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016629,
     "end_time": "2021-10-13T17:46:16.866632",
     "exception": false,
     "start_time": "2021-10-13T17:46:16.850003",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#balanced set (from training dataset)\n",
    "activations_name = \"weights0904-2d-30Hz-newinputs-dataaugmentation-30epochs_ALL\" #\"2_150_multiple_dropout_ALL\"\n",
    "saveFigs=False\n",
    "#further parameters: conservative, which units/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.166985,
     "end_time": "2021-10-13T17:46:17.041239",
     "exception": false,
     "start_time": "2021-10-13T17:46:16.874254",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "activations_name = \"weights_2_150_voiced_unvoiced_bis_ALL\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.114514,
     "end_time": "2021-10-13T17:46:17.178447",
     "exception": false,
     "start_time": "2021-10-13T17:46:17.063933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode_act=ALL_CELLS\n",
    "#corpus Ramus\n",
    "activations_folder_Ramus=f'../activations/Scores_Ramus/{activations_name}'\n",
    "#activations_folder_Ramus=\"../activations/Scores_Ramus/weights_2020-04-07\"\n",
    "\n",
    "metrics_folder='./corpus_ramus/Files/'\n",
    "\n",
    "activations_folder=f'../activations/Scores/{activations_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.6983,
     "end_time": "2021-10-13T17:46:18.899520",
     "exception": false,
     "start_time": "2021-10-13T17:46:17.201220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#In original code, d_act is used for activations on training set and datay used for Ramus corpus\n",
    "\n",
    "_, _, _, d_act, _, _, _ = create_dictionaries(activations_folder=activations_folder,\n",
    "                                                           metrics_folder=metrics_folder)\n",
    "\n",
    "_, d_match, datay, _, _, D, _ = create_dictionaries(activations_folder=activations_folder_Ramus,\n",
    "                                                           metrics_folder=metrics_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01157,
     "end_time": "2021-10-13T17:46:18.934097",
     "exception": false,
     "start_time": "2021-10-13T17:46:18.922527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Maps : single units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.018312,
     "end_time": "2021-10-13T17:46:18.960967",
     "exception": false,
     "start_time": "2021-10-13T17:46:18.942655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#cellule en abscisse\n",
    "xlstm='lstm_2'\n",
    "xcelltype='outputs'\n",
    "xcellnumber='35'\n",
    "xlabel=xlstm+\" \"+xcelltype+\" \"+xcellnumber\n",
    "\n",
    "#cellule en ordonnée\n",
    "ylstm='lstm_1'\n",
    "ycelltype='cell_states'\n",
    "ycellnumber='3'\n",
    "ylabel=ylstm+\" \"+ycelltype+\" \"+ycellnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.675357,
     "end_time": "2021-10-13T17:46:19.646725",
     "exception": false,
     "start_time": "2021-10-13T17:46:18.971368",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt=pl\n",
    "\n",
    "\n",
    "with pl.style.context('seaborn-deep'):\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "\n",
    "    #cellule en abscisse (fichier Ramus)\n",
    "    Daudiox={}\n",
    "    for langue in D:\n",
    "        Daudiox[langue]={}\n",
    "        for filename in D[langue]:\n",
    "            if filename in d_match:\n",
    "                fileaudio=d_match[filename]\n",
    "                activ=datay[fileaudio.split('.')[0]][xlstm][xcelltype][int(xcellnumber)]\n",
    "                Daudiox[langue][fileaudio]=float(activ)\n",
    "\n",
    "    #cellule en ordonnée (fichiers Ramus)\n",
    "    Daudioy={}\n",
    "    for langue in D:\n",
    "        Daudioy[langue]={}\n",
    "        for filename in D[langue]:\n",
    "            if filename in d_match:\n",
    "                fileaudio=d_match[filename]\n",
    "                activ=datay[fileaudio.split('.')[0]][ylstm][ycelltype][int(ycellnumber)]\n",
    "                Daudioy[langue][fileaudio]=float(activ)\n",
    "\n",
    "    #Figure\n",
    "    for langue in D: #en rouge les points correspondants aux fichiers de la base de F.Ramus\n",
    "        std=np.std(list(Daudiox[langue].values()))\n",
    "        xsterr=std/np.sqrt(len(list(Daudiox[langue].values())))\n",
    "        x=np.mean(list(Daudiox[langue].values()))\n",
    "        std=np.std(list(Daudioy[langue].values()))\n",
    "        ysterr=std/np.sqrt(len(list(Daudioy[langue].values())))\n",
    "        y=np.mean(list(Daudioy[langue].values()))\n",
    "        plt.errorbar(x,y,label=langue,fmt=\".r\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "        plt.text(x+0.0005,y+0.0005,langue)\n",
    "\n",
    "\n",
    "    langues={'English':'en','French':'fr','Polish':'pol','Japanese':'ja',\n",
    "             'Catalan':'cat','Spanish':'esp','Dutch':'du','Italian':'it'}\n",
    "\n",
    "\n",
    "    #cellule en abscisse\n",
    "    dlanguesx={}\n",
    "\n",
    "    for file in list(d_act.keys()):\n",
    "        language=d_act[file]['label']\n",
    "        if language not in dlanguesx:\n",
    "            dlanguesx[language]=[]\n",
    "        if mode_act == ALL_CELLS:\n",
    "            xcellnumber = int(xcellnumber)\n",
    "        activ=float(d_act[file]['activations'][xlstm][xcelltype][xcellnumber])\n",
    "        dlanguesx[language].append(activ)\n",
    "\n",
    "    #cellule en ordonnée   \n",
    "    dlanguesy={}\n",
    "\n",
    "    for file in list(d_act.keys()):\n",
    "        language=d_act[file]['label']\n",
    "        if language not in dlanguesy:\n",
    "            dlanguesy[language]=[]\n",
    "        if mode_act == ALL_CELLS:\n",
    "            ycellnumber = int(ycellnumber)\n",
    "        activ=float(d_act[file]['activations'][ylstm][ycelltype][ycellnumber])\n",
    "        dlanguesy[language].append(activ)\n",
    "\n",
    "    #Figure (fichiers hors de la base de F.Ramus)\n",
    "    for language in dlanguesy:\n",
    "        std=np.std(dlanguesx[language])\n",
    "        xsterr=std/(np.sqrt(len(dlanguesx[language])))\n",
    "        std=np.std(dlanguesy[language])\n",
    "        ysterr=std/(np.sqrt(len(dlanguesy[language])))\n",
    "        x=np.mean(dlanguesx[language])\n",
    "        y=np.mean(dlanguesy[language])\n",
    "        if language in langues: #en bleu les langues qui existent dans la base de F. Ramus\n",
    "            plt.errorbar(x,y,label=language,fmt=\".b\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "            plt.text(x+0.0001,y+0.0002,language)\n",
    "        else: #en noir les autres langues\n",
    "            plt.errorbar(x,y,label=language,fmt=\".k\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "            plt.text(x+0.0001,y+0.0002,language)\n",
    "\n",
    "\n",
    "    title='Distribution des langues selon '+xlabel+', '+ylabel\n",
    "    plt.title(title)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.xlabel(xlabel)\n",
    "    name=activations_name+'_distrib_'+xlabel.split(' ')[2]+'_'+ylabel.split(' ')[2]+'_ramus.svg'\n",
    "    if saveFigs:\n",
    "        plt.savefig(f'./corpus_ramus/Figures/{name}')\n",
    "    #plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011585,
     "end_time": "2021-10-13T17:46:19.680484",
     "exception": false,
     "start_time": "2021-10-13T17:46:19.668899",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Maps : combined units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langues={'English':'en','French':'fr','Polish':'pol','Japanese':'ja',\n",
    "             'Catalan':'cat','Spanish':'esp','Dutch':'du','Italian':'it'}\n",
    "langues_reversed = {v: k for k, v in langues.items()}\n",
    "    \n",
    "dic_lang_ramus={k: {} for k, v in langues.items()}  #to write means of correlates (Ramus corpus and DNN dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.008925,
     "end_time": "2021-10-13T17:46:19.698542",
     "exception": false,
     "start_time": "2021-10-13T17:46:19.689617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Import coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.020544,
     "end_time": "2021-10-13T17:46:19.727947",
     "exception": false,
     "start_time": "2021-10-13T17:46:19.707403",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conservative=True\n",
    "\n",
    "#activations_name=activations_folder_Ramus.split('/')[-1]\n",
    "json_filename=activations_name\n",
    "if conservative:\n",
    "    json_filename+='_conservative'\n",
    "with open(f'./corpus_ramus/coeffs/{json_filename}.json', 'r') as f:\n",
    "    coeffs_dic=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.130067,
     "end_time": "2021-10-13T17:46:19.869395",
     "exception": false,
     "start_time": "2021-10-13T17:46:19.739328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrique_x = 'nPVI_V'\n",
    "metrique_y = 'rPVI_C'\n",
    "layer='lstm_2'\n",
    "celltype='outputs'\n",
    "\n",
    "model = 'enet' #lasso #enet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "papermill": {
     "duration": 0.013111,
     "end_time": "2021-10-13T17:46:19.907305",
     "exception": false,
     "start_time": "2021-10-13T17:46:19.894194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "len(np.nonzero(coeffs_x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.661937,
     "end_time": "2021-10-13T17:46:20.583170",
     "exception": false,
     "start_time": "2021-10-13T17:46:19.921233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt=pl\n",
    "\n",
    "with pl.style.context('seaborn-deep'):\n",
    "    plt.figure(figsize=(10,8))\n",
    "\n",
    "    coeffs_x=np.array(coeffs_dic[metrique_x][model]['coeffs'])\n",
    "\n",
    "\n",
    "    intercept_x=coeffs_dic[metrique_x][model]['intercept']\n",
    "\n",
    "    coeffs_y=np.array(coeffs_dic[metrique_y][model]['coeffs'])\n",
    "\n",
    "    intercept_y=coeffs_dic[metrique_y][model]['intercept']\n",
    "\n",
    "    if metrique_x in ['deltC', 'deltV']:\n",
    "        shift_x=0.00002\n",
    "    elif metrique_x in ['propV', 'nPVI_V', 'rPVI_C']:\n",
    "        shift_x=0.02\n",
    "    elif metrique_x in ['rPVI_C']:\n",
    "        shift_x=0.002\n",
    "\n",
    "    if metrique_y in ['deltC', 'deltV']:\n",
    "        shift_y=0.00005\n",
    "    elif metrique_y in ['propV',  'nPVI_V']:\n",
    "        shift_y=0.05\n",
    "    elif metrique_y in ['rPVI_C']:\n",
    "        shift_y=0.005\n",
    "\n",
    "    xlabel=metrique_x\n",
    "    ylabel=metrique_y\n",
    "\n",
    "    #fichiers Ramus\n",
    "    Daudiox={}\n",
    "    Daudioy={}\n",
    "    for langue in D:\n",
    "        Daudiox[langue]={}\n",
    "        Daudioy[langue]={}\n",
    "        for filename in D[langue]:\n",
    "            if filename in d_match:\n",
    "                fileaudio=d_match[filename]\n",
    "                activ=np.array([float(st) for st in datay[fileaudio.split('.')[0]][layer][celltype]])\n",
    "                Daudiox[langue][fileaudio]=intercept_x + np.dot(coeffs_x, activ)\n",
    "                Daudioy[langue][fileaudio]=intercept_y + np.dot(coeffs_y, activ)\n",
    "                \n",
    "\n",
    "    #Figure\n",
    "    for langue in D: #en rouge les points correspondants aux fichiers de la base de F.Ramus\n",
    "        std=np.std(list(Daudiox[langue].values()))\n",
    "        xsterr=std/np.sqrt(len(list(Daudiox[langue].values())))\n",
    "        x=np.mean(list(Daudiox[langue].values()))\n",
    "        std=np.std(list(Daudioy[langue].values()))\n",
    "        ysterr=std/np.sqrt(len(list(Daudioy[langue].values())))\n",
    "        y=np.mean(list(Daudioy[langue].values()))\n",
    "        plt.errorbar(x,y,label=langue,xerr=xsterr,yerr=ysterr,capsize=2, ecolor='orangered')\n",
    "        plt.text(x+shift_x,y+shift_y,langue)\n",
    "        \n",
    "        dic_lang_ramus[langues_reversed[langue]][f'ramus_{metrique_x}']=x\n",
    "        dic_lang_ramus[langues_reversed[langue]][f'ramus_{metrique_y}']=y\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    #cellule en abscisse\n",
    "    dlanguesx={}\n",
    "    dlanguesy={}\n",
    "\n",
    "    for file in list(d_act.keys()):\n",
    "        language=d_act[file]['label']\n",
    "        if language not in dlanguesx:\n",
    "            dlanguesx[language]=[]\n",
    "            dlanguesy[language]=[]\n",
    "        activ=np.array([float(st) for st in d_act[file]['activations'][layer][celltype]])\n",
    "        dlanguesx[language].append(intercept_x + np.dot(coeffs_x, activ))\n",
    "        dlanguesy[language].append(intercept_y + np.dot(coeffs_y, activ))\n",
    "\n",
    "    #Figure (fichiers hors de la base de F.Ramus)\n",
    "    for language in dlanguesy:\n",
    "        std=np.std(dlanguesx[language])\n",
    "        xsterr=std/(np.sqrt(len(dlanguesx[language])))\n",
    "        std=np.std(dlanguesy[language])\n",
    "        ysterr=std/(np.sqrt(len(dlanguesy[language])))\n",
    "        x=np.mean(dlanguesx[language])\n",
    "        y=np.mean(dlanguesy[language])\n",
    "\n",
    "        if language in langues: #en bleu les langues qui existent dans la base de F. Ramus\n",
    "            plt.errorbar(x,y,label=language,xerr=xsterr,yerr=ysterr,capsize=2, ecolor='blue')\n",
    "            plt.text(x+shift_x,y+shift_y,language)\n",
    "            \n",
    "            \n",
    "            dic_lang_ramus[language][f'DNN_{metrique_x}']=x\n",
    "            dic_lang_ramus[language][f'DNN_{metrique_y}']=y\n",
    "        else: #en noir les autres langues\n",
    "            plt.errorbar(x,y,label=language,xerr=xsterr,yerr=ysterr,capsize=2, ecolor='black')\n",
    "            plt.text(x+shift_x,y+shift_y,language)\n",
    "\n",
    "    title='Distribution des langues selon les activations corrélées à '+xlabel+', '+ylabel + f' ({model})'\n",
    "    #plt.title(title)\n",
    "    if ylabel=='deltC':\n",
    "        #pl.ylabel('Correlated with $\\Delta C$')\n",
    "        pl.ylabel('Correlated with ')\n",
    "    else:\n",
    "        plt.ylabel(ylabel)\n",
    "\n",
    "    if xlabel=='propV' or xlabel=='deltV':\n",
    "        pl.xlabel('Correlated with ')\n",
    "    else:\n",
    "        plt.xlabel(xlabel)\n",
    "    name=activations_name+'_distrib_'+xlabel+'_'+ylabel+'_'+model\n",
    "    if conservative:\n",
    "        name=name+\"_conservative\"\n",
    "    name=name+'.svg'\n",
    "    if saveFigs:\n",
    "        plt.savefig(f'./corpus_ramus/Figures/{name}')\n",
    "    #plt.close()\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_lang_ramus"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#write csv for dic_lang_ramus (from chatGPT)\n",
    "\n",
    "# Extract column labels (metrics)\n",
    "columns = set(metric for metrics in dic_lang_ramus.values() for metric in metrics.keys())\n",
    "columns = ['Language'] + sorted(columns)\n",
    "\n",
    "# Write to CSV file\n",
    "csv_file_path = 'metrics_data.csv'\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=columns)\n",
    "    \n",
    "    # Write header row\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write data rows\n",
    "    for language, metrics in dic_lang_ramus.items():\n",
    "        row_data = {'Language': language, **metrics}\n",
    "        writer.writerow(row_data)\n",
    "\n",
    "print(f'CSV file written successfully at: {csv_file_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011888,
     "end_time": "2021-10-13T17:46:20.617482",
     "exception": false,
     "start_time": "2021-10-13T17:46:20.605594",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Classification based on feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.009866,
     "end_time": "2021-10-13T17:46:20.637215",
     "exception": false,
     "start_time": "2021-10-13T17:46:20.627349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Features array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.994083,
     "end_time": "2021-10-13T17:46:21.641025",
     "exception": false,
     "start_time": "2021-10-13T17:46:20.646942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_labels=[] #languages\n",
    "metric_names=['propV', 'deltC', 'nPVI_V', 'rPVI_C']\n",
    "array_metrics=[] #as list of list for construction\n",
    "list_scores_nn=[] #List scores (neural network): scores as dict lang -> score\n",
    "for file in list(d_act.keys()):\n",
    "    language=d_act[file]['label']\n",
    "    \n",
    "    if language not in ['Czech', 'Romanian']: #not in list of model languages\n",
    "        features=[]\n",
    "        list_labels.append(language)\n",
    "        activ=np.array([float(st) for st in d_act[file]['activations'][layer][celltype]])\n",
    "        for metric in metric_names:\n",
    "            intercept_x=coeffs_dic[metric][model]['intercept']\n",
    "            coeffs_x=np.array(coeffs_dic[metric][model]['coeffs'])\n",
    "            feature=intercept_x + np.dot(coeffs_x, activ)\n",
    "            features.append(feature)\n",
    "        array_metrics.append(features)\n",
    "        \n",
    "        \n",
    "        with open(f'{activations_folder}/{file}', 'r') as json_file:\n",
    "            data_json=json.load(json_file)\n",
    "            list_scores_nn.append(data_json[\"scores\"])\n",
    "            \n",
    "            \n",
    "\n",
    "array_metrics=np.array(array_metrics)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012376,
     "end_time": "2021-10-13T17:46:21.679717",
     "exception": false,
     "start_time": "2021-10-13T17:46:21.667341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.018027,
     "end_time": "2021-10-13T17:46:21.707712",
     "exception": false,
     "start_time": "2021-10-13T17:46:21.689685",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#languages=set(list_labels)\n",
    "languages=list(list_scores_nn[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.167826,
     "end_time": "2021-10-13T17:46:21.885834",
     "exception": false,
     "start_time": "2021-10-13T17:46:21.718008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target=[]\n",
    "scores_nn_arr=[]\n",
    "target_multarr=np.zeros((len(list_labels), len(languages)))\n",
    "i=0\n",
    "for label, scores in zip(list_labels, list_scores_nn):\n",
    "    ind=languages.index(label)\n",
    "    target.append(ind)\n",
    "    target_multarr[i, ind]=1\n",
    "    scores_nn_arr.append([float(scores[lang]) for lang in languages])\n",
    "target=np.array(target)\n",
    "scores_nn_arr=np.array(scores_nn_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012467,
     "end_time": "2021-10-13T17:46:21.923041",
     "exception": false,
     "start_time": "2021-10-13T17:46:21.910574",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "QDA with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.250545,
     "end_time": "2021-10-13T17:46:22.183814",
     "exception": false,
     "start_time": "2021-10-13T17:46:21.933269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from scipy import linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.020446,
     "end_time": "2021-10-13T17:46:22.230077",
     "exception": false,
     "start_time": "2021-10-13T17:46:22.209631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "QDA=QuadraticDiscriminantAnalysis\n",
    "model_qda=QDA(store_covariance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.18621,
     "end_time": "2021-10-13T17:46:22.426584",
     "exception": false,
     "start_time": "2021-10-13T17:46:22.240374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_qda.fit(array_metrics, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qda.score(array_metrics, target)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#save model\n",
    "from joblib import dump, load\n",
    "dump(model_qda, 'qda.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.309284,
     "end_time": "2021-10-13T17:46:22.759537",
     "exception": false,
     "start_time": "2021-10-13T17:46:22.450253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#plot elipses\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, lang, axes=(0,1)):\n",
    "    cov=np.array([[cov[axes[0], axes[0]], cov[axes[0], axes[1]] ],\n",
    "                       [cov[axes[1], axes[0]], cov[axes[1], axes[1]] ]])\n",
    "    mean=np.array([mean[axes[0]], mean[axes[1]]])                    \n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at X standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 0.2 * v[0] ** 0.5, 0.2 * v[1] ** 0.5,\n",
    "                              180 + angle, # facecolor=color,\n",
    "                              edgecolor='black', linewidth=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    #ell.set_alpha(0.2)\n",
    "    #splot.scatter(mean[0], mean[1])\n",
    "    splot.add_artist(ell)\n",
    "    #splot.set_xticks(())\n",
    "    #splot.set_yticks(())\n",
    "\n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "splot=pl.gca()\n",
    "for lang, mean, cov in zip(languages, model_qda.means_, model_qda.covariance_):\n",
    "    axes=(2,3)\n",
    "    mean2=np.array([mean[axes[0]], mean[axes[1]]])\n",
    "    pl.scatter(mean2[0], mean2[1])\n",
    "    pl.text(mean2[0], mean2[1], lang)\n",
    "    \n",
    "    plot_ellipse(splot, mean, cov, lang, axes=axes)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01289,
     "end_time": "2021-10-13T17:46:22.798772",
     "exception": false,
     "start_time": "2021-10-13T17:46:22.785882",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.604147,
     "end_time": "2021-10-13T17:46:23.413849",
     "exception": false,
     "start_time": "2021-10-13T17:46:22.809702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, top_k_accuracy_score #requires scikit-learn 0.24\n",
    "scores_cv_acc = cross_val_score(model_qda, array_metrics, target, cv=5)\n",
    "   \n",
    "scores_cv_top_3 = cross_val_score(model_qda, array_metrics, target, cv=5, \n",
    "             scoring=make_scorer(top_k_accuracy_score, k=3, needs_proba=True, labels=np.arange(len(languages))) )\n",
    "\n",
    "print(f'cross-validation accuracy: {np.mean(scores_cv_acc):.4f} ({scores_cv_acc})')\n",
    "\n",
    "print(f'cross-validation top3 accuracy: {np.mean(scores_cv_top_3):.4f} ({scores_cv_top_3})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.185181,
     "end_time": "2021-10-13T17:46:23.720453",
     "exception": false,
     "start_time": "2021-10-13T17:46:23.535272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Measures of information (w/ fit on all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.745906,
     "end_time": "2021-10-13T17:46:27.535402",
     "exception": false,
     "start_time": "2021-10-13T17:46:23.789496",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_ent_nn=0\n",
    "cross_ent_qda=0\n",
    "acc_nn=0\n",
    "jsd=0\n",
    "tv=0\n",
    "for i in range(len(list_labels)):\n",
    "    scores_nn=scores_nn_arr[i]\n",
    "    \n",
    "    features=array_metrics[i]\n",
    "    scores_qda=model_qda.predict_proba(features[np.newaxis, :])[0]\n",
    "    \n",
    "    ind=target[i]\n",
    "    \n",
    "    ind_nn=np.argmax(scores_nn)\n",
    "    acc_nn+= (ind_nn)==ind\n",
    "    \n",
    "    cross_ent_nn+=-np.log2(scores_nn[ind])\n",
    "    cross_ent_qda+=-np.log2(scores_qda[ind])\n",
    "    jsd+=-0.5*np.sum(scores_nn*np.log2( (1+scores_qda/scores_nn)/2))\n",
    "    jsd+=-0.5*np.sum(scores_qda*np.log2( (1+scores_nn/scores_qda)/2))\n",
    "    tv+=np.sum(np.abs(scores_nn-scores_qda))\n",
    "    \n",
    "    if i>10 and i<20:\n",
    "        pl.subplot(10, 1, i-10)\n",
    "        pl.bar(range(len(languages)), scores_nn)\n",
    "        pl.bar(np.arange(len(languages))+0.5, scores_qda)\n",
    "cross_ent_nn/=len(list_labels)\n",
    "cross_ent_qda/=len(list_labels)\n",
    "acc_nn/=len(list_labels)\n",
    "jsd/=len(list_labels)\n",
    "tv/=len(list_labels)\n",
    "\n",
    "print(f'cross entropy NN :{cross_ent_nn:.4f} (perplexity {2**cross_ent_nn:.4f})')\n",
    "print(f'accuracy NN :{acc_nn:.2f}')    \n",
    "print(f'cross entropy QDA :{cross_ent_qda:.4f} (perplexity {2**cross_ent_qda:.4f})')\n",
    "\n",
    "print(f'mean jensen-shannon divergence :{jsd:.4f} bit')\n",
    "\n",
    "print(f'mean total variation :{tv:.3f} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.011986,
     "end_time": "2021-10-13T17:46:27.600841",
     "exception": false,
     "start_time": "2021-10-13T17:46:27.588855",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.428386,
   "end_time": "2021-10-13T17:46:28.223577",
   "environment_variables": {},
   "exception": null,
   "input_path": "Analyze activations 2 - Maps languages.ipynb",
   "output_path": "2_150_bis - Analyze activations 2.ipynb",
   "parameters": {
    "activations_name": "weights_2_150_voiced_unvoiced_bis_ALL"
   },
   "start_time": "2021-10-13T17:46:15.795191",
   "version": "2.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
