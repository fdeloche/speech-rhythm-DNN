{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['text.usetex'] = True\n",
    "\n",
    "mpl.rcParams['axes.titlesize']=24\n",
    "mpl.rcParams['axes.labelsize']= 20\n",
    "mpl.rcParams['lines.linewidth']= 3\n",
    "mpl.rcParams['font.size']= 16\n",
    "mpl.rcParams['lines.markersize']=  10\n",
    "mpl.rcParams['xtick.labelsize']=  16\n",
    "mpl.rcParams['ytick.labelsize']=  16\n",
    "    \n",
    "import matplotlib.pyplot as pl\n",
    "from scipy.stats.stats import pearsonr\n",
    "from create_dictionaries import create_dictionaries\n",
    "import copy\n",
    "\n",
    "import json\n",
    "\n",
    "ALL_CELLS=0\n",
    "SELECTED_CELLS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#balanced set (from training dataset)\n",
    "activations_name = \"weights0904-2d-30Hz-newinputs-dataaugmentation-30epochs_ALL\" #\"2_150_multiple_dropout_ALL\"\n",
    "saveFigs=False\n",
    "#further parameters: conservative, which units/metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_act=ALL_CELLS\n",
    "#corpus Ramus\n",
    "activations_folder_Ramus=f'../activations/Scores_Ramus/{activations_name}'\n",
    "#activations_folder_Ramus=\"../activations/Scores_Ramus/weights_2020-04-07\"\n",
    "\n",
    "metrics_folder='./corpus_ramus/Files/'\n",
    "\n",
    "activations_folder=f'../activations/Scores/{activations_name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In original code, d_act is used for activations on training set and datay used for Ramus corpus\n",
    "\n",
    "_, _, _, d_act, _, _, _ = create_dictionaries(activations_folder=activations_folder,\n",
    "                                                           metrics_folder=metrics_folder)\n",
    "\n",
    "_, d_match, datay, _, _, D, _ = create_dictionaries(activations_folder=activations_folder_Ramus,\n",
    "                                                           metrics_folder=metrics_folder)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps : single units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cellule en abscisse\n",
    "xlstm='lstm_2'\n",
    "xcelltype='outputs'\n",
    "xcellnumber='119'\n",
    "xlabel=xlstm+\" \"+xcelltype+\" \"+xcellnumber\n",
    "\n",
    "#cellule en ordonnée\n",
    "ylstm='lstm_2'\n",
    "ycelltype='outputs'\n",
    "ycellnumber='64'\n",
    "ylabel=ylstm+\" \"+ycelltype+\" \"+ycellnumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt=pl\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "#cellule en abscisse (fichier Ramus)\n",
    "Daudiox={}\n",
    "for langue in D:\n",
    "    Daudiox[langue]={}\n",
    "    for filename in D[langue]:\n",
    "        if filename in d_match:\n",
    "            fileaudio=d_match[filename]\n",
    "            activ=datay[fileaudio.split('.')[0]][xlstm][xcelltype][int(xcellnumber)]\n",
    "            Daudiox[langue][fileaudio]=float(activ)\n",
    "\n",
    "#cellule en ordonnée (fichiers Ramus)\n",
    "Daudioy={}\n",
    "for langue in D:\n",
    "    Daudioy[langue]={}\n",
    "    for filename in D[langue]:\n",
    "        if filename in d_match:\n",
    "            fileaudio=d_match[filename]\n",
    "            activ=datay[fileaudio.split('.')[0]][ylstm][ycelltype][int(ycellnumber)]\n",
    "            Daudioy[langue][fileaudio]=float(activ)\n",
    "\n",
    "#Figure\n",
    "for langue in D: #en rouge les points correspondants aux fichiers de la base de F.Ramus\n",
    "    std=np.std(list(Daudiox[langue].values()))\n",
    "    xsterr=std/np.sqrt(len(list(Daudiox[langue].values())))\n",
    "    x=np.mean(list(Daudiox[langue].values()))\n",
    "    std=np.std(list(Daudioy[langue].values()))\n",
    "    ysterr=std/np.sqrt(len(list(Daudioy[langue].values())))\n",
    "    y=np.mean(list(Daudioy[langue].values()))\n",
    "    plt.errorbar(x,y,label=langue,fmt=\".r\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "    plt.text(x+0.0005,y+0.0005,langue)\n",
    "\n",
    "\n",
    "langues={'English':'en','French':'fr','Polish':'pol','Japanese':'ja',\n",
    "         'Catalan':'cat','Spanish':'esp','Dutch':'du','Italian':'it'}\n",
    "\n",
    "\n",
    "#cellule en abscisse\n",
    "dlanguesx={}\n",
    "\n",
    "for file in list(d_act.keys()):\n",
    "    language=d_act[file]['label']\n",
    "    if language not in dlanguesx:\n",
    "        dlanguesx[language]=[]\n",
    "    if mode_act == ALL_CELLS:\n",
    "        xcellnumber = int(xcellnumber)\n",
    "    activ=float(d_act[file]['activations'][xlstm][xcelltype][xcellnumber])\n",
    "    dlanguesx[language].append(activ)\n",
    "\n",
    "#cellule en ordonnée   \n",
    "dlanguesy={}\n",
    "\n",
    "for file in list(d_act.keys()):\n",
    "    language=d_act[file]['label']\n",
    "    if language not in dlanguesy:\n",
    "        dlanguesy[language]=[]\n",
    "    if mode_act == ALL_CELLS:\n",
    "        ycellnumber = int(ycellnumber)\n",
    "    activ=float(d_act[file]['activations'][ylstm][ycelltype][ycellnumber])\n",
    "    dlanguesy[language].append(activ)\n",
    "\n",
    "#Figure (fichiers hors de la base de F.Ramus)\n",
    "for language in dlanguesy:\n",
    "    std=np.std(dlanguesx[language])\n",
    "    xsterr=std/(np.sqrt(len(dlanguesx[language])))\n",
    "    std=np.std(dlanguesy[language])\n",
    "    ysterr=std/(np.sqrt(len(dlanguesy[language])))\n",
    "    x=np.mean(dlanguesx[language])\n",
    "    y=np.mean(dlanguesy[language])\n",
    "    if language in langues: #en bleu les langues qui existent dans la base de F. Ramus\n",
    "        plt.errorbar(x,y,label=language,fmt=\".b\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "        plt.text(x+0.0001,y+0.0002,language)\n",
    "    else: #en noir les autres langues\n",
    "        plt.errorbar(x,y,label=language,fmt=\".k\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "        plt.text(x+0.0001,y+0.0002,language)\n",
    "\n",
    "\n",
    "title='Distribution des langues selon '+xlabel+', '+ylabel\n",
    "plt.title(title)\n",
    "plt.ylabel(ylabel)\n",
    "plt.xlabel(xlabel)\n",
    "name=activations_name+'_distrib_'+xlabel.split(' ')[2]+'_'+ylabel.split(' ')[2]+'_ramus.png'\n",
    "if saveFigs:\n",
    "    plt.savefig(f'./corpus_ramus/Figures/{name}')\n",
    "#plt.close()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maps : combined units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conservative=False\n",
    "\n",
    "#activations_name=activations_folder_Ramus.split('/')[-1]\n",
    "json_filename=activations_name\n",
    "if conservative:\n",
    "    json_filename+='_conservative'\n",
    "with open(f'./corpus_ramus/coeffs/{json_filename}.json', 'r') as f:\n",
    "    coeffs_dic=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrique_x = 'propV'\n",
    "metrique_y = 'deltC'\n",
    "layer='lstm_2'\n",
    "celltype='outputs'\n",
    "\n",
    "model = 'lasso' #lasso #enet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.nonzero(coeffs_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt=pl\n",
    "plt.figure(figsize=(10,8))\n",
    "\n",
    "coeffs_x=np.array(coeffs_dic[metrique_x][model]['coeffs'])\n",
    "\n",
    "\n",
    "intercept_x=coeffs_dic[metrique_x][model]['intercept']\n",
    "\n",
    "coeffs_y=np.array(coeffs_dic[metrique_y][model]['coeffs'])\n",
    "\n",
    "intercept_y=coeffs_dic[metrique_y][model]['intercept']\n",
    "\n",
    "if metrique_x in ['deltC', 'deltV']:\n",
    "    shift_x=0.00002\n",
    "elif metrique_x in ['propV', 'nPVI_V', 'rPVI_C']:\n",
    "    shift_x=0.02\n",
    "elif metrique_x in ['rPVI_C']:\n",
    "    shift_x=0.002\n",
    "\n",
    "if metrique_y in ['deltC', 'deltV']:\n",
    "    shift_y=0.00005\n",
    "elif metrique_y in ['propV',  'nPVI_V']:\n",
    "    shift_y=0.05\n",
    "elif metrique_y in ['rPVI_C']:\n",
    "    shift_y=0.005\n",
    "\n",
    "xlabel=metrique_x\n",
    "ylabel=metrique_y\n",
    "        \n",
    "#fichiers Ramus\n",
    "Daudiox={}\n",
    "Daudioy={}\n",
    "for langue in D:\n",
    "    Daudiox[langue]={}\n",
    "    Daudioy[langue]={}\n",
    "    for filename in D[langue]:\n",
    "        if filename in d_match:\n",
    "            fileaudio=d_match[filename]\n",
    "            activ=np.array([float(st) for st in datay[fileaudio.split('.')[0]][layer][celltype]])\n",
    "            Daudiox[langue][fileaudio]=intercept_x + np.dot(coeffs_x, activ)\n",
    "            Daudioy[langue][fileaudio]=intercept_y + np.dot(coeffs_y, activ)\n",
    "\n",
    "#Figure\n",
    "for langue in D: #en rouge les points correspondants aux fichiers de la base de F.Ramus\n",
    "    std=np.std(list(Daudiox[langue].values()))\n",
    "    xsterr=std/np.sqrt(len(list(Daudiox[langue].values())))\n",
    "    x=np.mean(list(Daudiox[langue].values()))\n",
    "    std=np.std(list(Daudioy[langue].values()))\n",
    "    ysterr=std/np.sqrt(len(list(Daudioy[langue].values())))\n",
    "    y=np.mean(list(Daudioy[langue].values()))\n",
    "    plt.errorbar(x,y,label=langue,fmt=\".r\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "    plt.text(x+shift_x,y+shift_y,langue)\n",
    "\n",
    "\n",
    "langues={'English':'en','French':'fr','Polish':'pol','Japanese':'ja',\n",
    "         'Catalan':'cat','Spanish':'esp','Dutch':'du','Italian':'it'}\n",
    "\n",
    "\n",
    "#cellule en abscisse\n",
    "dlanguesx={}\n",
    "dlanguesy={}\n",
    "\n",
    "for file in list(d_act.keys()):\n",
    "    language=d_act[file]['label']\n",
    "    if language not in dlanguesx:\n",
    "        dlanguesx[language]=[]\n",
    "        dlanguesy[language]=[]\n",
    "    activ=np.array([float(st) for st in d_act[file]['activations'][layer][celltype]])\n",
    "    dlanguesx[language].append(intercept_x + np.dot(coeffs_x, activ))\n",
    "    dlanguesy[language].append(intercept_y + np.dot(coeffs_y, activ))\n",
    "\n",
    "#Figure (fichiers hors de la base de F.Ramus)\n",
    "for language in dlanguesy:\n",
    "    std=np.std(dlanguesx[language])\n",
    "    xsterr=std/(np.sqrt(len(dlanguesx[language])))\n",
    "    std=np.std(dlanguesy[language])\n",
    "    ysterr=std/(np.sqrt(len(dlanguesy[language])))\n",
    "    x=np.mean(dlanguesx[language])\n",
    "    y=np.mean(dlanguesy[language])\n",
    "    \n",
    "    if language in langues: #en bleu les langues qui existent dans la base de F. Ramus\n",
    "        plt.errorbar(x,y,label=language,fmt=\".b\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "        plt.text(x+shift_x,y+shift_y,language)\n",
    "    else: #en noir les autres langues\n",
    "        plt.errorbar(x,y,label=language,fmt=\".k\",xerr=xsterr,yerr=ysterr,capsize=2)\n",
    "        plt.text(x+shift_x,y+shift_y,language)\n",
    "\n",
    "title='Distribution des langues selon les activations corrélées à '+xlabel+', '+ylabel + f' ({model})'\n",
    "plt.title(title)\n",
    "plt.ylabel(ylabel)\n",
    "plt.xlabel(xlabel)\n",
    "name=activations_name+'_distrib_'+xlabel+'_'+ylabel+'_'+model\n",
    "if conservative:\n",
    "    name=name+\"_conservative\"\n",
    "name=name+'.png'\n",
    "if saveFigs:\n",
    "    plt.savefig(f'./corpus_ramus/Figures/{name}')\n",
    "#plt.close()\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification based on feature maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels=[] #languages\n",
    "metric_names=['propV', 'deltC', 'nPVI_V', 'rPVI_C']\n",
    "array_metrics=[] #as list of list for construction\n",
    "list_scores_nn=[] #List scores (neural network): scores as dict lang -> score\n",
    "for file in list(d_act.keys()):\n",
    "    language=d_act[file]['label']\n",
    "    \n",
    "    if language not in ['Czech', 'Romanian']: #not in list of model languages\n",
    "        features=[]\n",
    "        list_labels.append(language)\n",
    "        activ=np.array([float(st) for st in d_act[file]['activations'][layer][celltype]])\n",
    "        for metric in metric_names:\n",
    "            intercept_x=coeffs_dic[metric][model]['intercept']\n",
    "            coeffs_x=np.array(coeffs_dic[metric][model]['coeffs'])\n",
    "            feature=intercept_x + np.dot(coeffs_x, activ)\n",
    "            features.append(feature)\n",
    "        array_metrics.append(features)\n",
    "        \n",
    "        \n",
    "        with open(f'{activations_folder}/{file}', 'r') as json_file:\n",
    "            data_json=json.load(json_file)\n",
    "            list_scores_nn.append(data_json[\"scores\"])\n",
    "            \n",
    "            \n",
    "\n",
    "array_metrics=np.array(array_metrics)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#languages=set(list_labels)\n",
    "languages=list(list_scores_nn[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=[]\n",
    "scores_nn_arr=[]\n",
    "target_multarr=np.zeros((len(list_labels), len(languages)))\n",
    "i=0\n",
    "for label, scores in zip(list_labels, list_scores_nn):\n",
    "    ind=languages.index(label)\n",
    "    target.append(ind)\n",
    "    target_multarr[i, ind]=1\n",
    "    scores_nn_arr.append([float(scores[lang]) for lang in languages])\n",
    "target=np.array(target)\n",
    "scores_nn_arr=np.array(scores_nn_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QDA with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "from scipy import linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QDA=QuadraticDiscriminantAnalysis\n",
    "model_qda=QDA(store_covariance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_qda.fit(array_metrics, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#plot elipses\n",
    "\n",
    "def plot_ellipse(splot, mean, cov, lang, axes=(0,1)):\n",
    "    cov=np.array([[cov[axes[0], axes[0]], cov[axes[0], axes[1]] ],\n",
    "                       [cov[axes[1], axes[0]], cov[axes[1], axes[1]] ]])\n",
    "    mean=np.array([mean[axes[0]], mean[axes[1]]])                    \n",
    "    v, w = linalg.eigh(cov)\n",
    "    u = w[0] / linalg.norm(w[0])\n",
    "    angle = np.arctan(u[1] / u[0])\n",
    "    angle = 180 * angle / np.pi  # convert to degrees\n",
    "    # filled Gaussian at X standard deviation\n",
    "    ell = mpl.patches.Ellipse(mean, 0.2 * v[0] ** 0.5, 0.2 * v[1] ** 0.5,\n",
    "                              180 + angle, # facecolor=color,\n",
    "                              edgecolor='black', linewidth=2)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    #ell.set_alpha(0.2)\n",
    "    #splot.scatter(mean[0], mean[1])\n",
    "    splot.add_artist(ell)\n",
    "    #splot.set_xticks(())\n",
    "    #splot.set_yticks(())\n",
    "\n",
    "\n",
    "pl.figure(figsize=(10,10))\n",
    "splot=pl.gca()\n",
    "for lang, mean, cov in zip(languages, model_qda.means_, model_qda.covariance_):\n",
    "    axes=(0,1)\n",
    "    mean=np.array([mean[axes[0]], mean[axes[1]]])\n",
    "    pl.scatter(mean[0], mean[1])\n",
    "    pl.text(mean[0], mean[1], lang)\n",
    "    \n",
    "    plot_ellipse(splot, mean, cov, lang, axes=axes)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, top_k_accuracy_score #requires scikit-learn 0.24\n",
    "scores_cv_acc = cross_val_score(model_qda, array_metrics, target, cv=5)\n",
    "   \n",
    "scores_cv_top_3 = cross_val_score(model_qda, array_metrics, target, cv=5, \n",
    "             scoring=make_scorer(top_k_accuracy_score, k=3, needs_proba=True, labels=np.arange(len(languages))) )\n",
    "\n",
    "print(f'cross-validation accuracy: {np.mean(scores_cv_acc):.4f} ({scores_cv_acc})')\n",
    "\n",
    "print(f'cross-validation top3 accuracy: {np.mean(scores_cv_top_3):.4f} ({scores_cv_top_3})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measures of information (w/ fit on all data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_ent_nn=0\n",
    "cross_ent_qda=0\n",
    "acc_nn=0\n",
    "jsd=0\n",
    "tv=0\n",
    "for i in range(len(list_labels)):\n",
    "    scores_nn=scores_nn_arr[i]\n",
    "    \n",
    "    features=array_metrics[i]\n",
    "    scores_qda=model_qda.predict_proba(features[np.newaxis, :])[0]\n",
    "    \n",
    "    ind=target[i]\n",
    "    \n",
    "    ind_nn=np.argmax(scores_nn)\n",
    "    acc_nn+= (ind_nn)==ind\n",
    "    \n",
    "    cross_ent_nn+=-np.log2(scores_nn[ind])\n",
    "    cross_ent_qda+=-np.log2(scores_qda[ind])\n",
    "    jsd+=-0.5*np.sum(scores_nn*np.log2( (1+scores_qda/scores_nn)/2))\n",
    "    jsd+=-0.5*np.sum(scores_qda*np.log2( (1+scores_nn/scores_qda)/2))\n",
    "    tv+=np.sum(np.abs(scores_nn-scores_qda))\n",
    "    \n",
    "    if i<10:\n",
    "        pl.subplot(10, 1, i+1)\n",
    "        pl.bar(range(len(languages)), scores_nn)\n",
    "        pl.bar(range(len(languages)), scores_qda)\n",
    "cross_ent_nn/=len(list_labels)\n",
    "cross_ent_qda/=len(list_labels)\n",
    "acc_nn/=len(list_labels)\n",
    "jsd/=len(list_labels)\n",
    "tv/=len(list_labels)\n",
    "\n",
    "print(f'cross entropy NN :{cross_ent_nn:.4f} (perplexity {2**cross_ent_nn:.4f})')\n",
    "print(f'accuracy NN :{acc_nn:.2f}')    \n",
    "print(f'cross entropy QDA :{cross_ent_qda:.4f} (perplexity {2**cross_ent_qda:.4f})')\n",
    "\n",
    "print(f'mean jensen-shannon divergence :{jsd:.4f} bit')\n",
    "\n",
    "print(f'mean total variation :{tv:.3f} ')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
